{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME.bak","path":"CNAME.bak","modified":0,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/muse.js","path":"js/src/schemes/muse.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1}],"Cache":[{"_id":"source/CNAME.bak","hash":"cf67bbbfbd1ce091afeba3a3208c7f218826df71","modified":1550629419433},{"_id":"themes/next/.editorconfig","hash":"211d2c92bfdddb3e81ea946f4ca7a539f150f4da","modified":1550629419494},{"_id":"themes/next/.all-contributorsrc","hash":"0ca0200bb5cba4cd157f2cb74703c4cb4c29cda5","modified":1550629419493},{"_id":"themes/next/.bowerrc","hash":"334da94ca6f024d60d012cc26ea655681e724ad8","modified":1550629419493},{"_id":"themes/next/.stylintrc","hash":"3b7f9785e9ad0dab764e1c535b40df02f4ff5fd6","modified":1550629419495},{"_id":"themes/next/.gitattributes","hash":"8454b9313cb1a97b63fb87e2d29daee497ce6249","modified":1550629419495},{"_id":"themes/next/.travis.yml","hash":"fb9ac54e875f6ea16d5c83db497f6bd70ae83198","modified":1550629419496},{"_id":"themes/next/.eslintrc.json","hash":"d3c11de434171d55d70daadd3914bc33544b74b8","modified":1550629419494},{"_id":"themes/next/LICENSE.md","hash":"0a9c7399f102b4eb0a6950dd31264be421557c7d","modified":1550629419496},{"_id":"themes/next/_config.yml","hash":"1a1a9b7063499590fc029ae67f18530d3da799b3","modified":1550629419498},{"_id":"themes/next/README.md","hash":"943f9fd6ed1781350cdd05a26a1cfad24d035c8b","modified":1550629419497},{"_id":"themes/next/crowdin.yml","hash":"4a53f5985e545c635cb56b2a57ed290cb8cf8942","modified":1550629419499},{"_id":"themes/next/bower.json","hash":"b17bf6ad5dd98d60129a0827b48b59f2f349b7be","modified":1550629419498},{"_id":"themes/next/gulpfile.coffee","hash":"67eaf2515100971f6195b60eeebbfe5e8de895ab","modified":1550629419513},{"_id":"themes/next/package.json","hash":"159d8e0a65ad1f9457745e4864d6a882238d1df7","modified":1550629419578},{"_id":"source/_posts/Untitled.md","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1556545343724},{"_id":"source/categories/index.md","hash":"e08517392b75074a28805bc6429641fb2cf061b0","modified":1550629419440},{"_id":"source/tags/index.md","hash":"4c0de90f5ed750d70210286fb157d49f2647e245","modified":1550629419441},{"_id":"themes/next/docs/AGPL3.md","hash":"f463f95b169d64983f59fa6f3e4b6760290a0e6b","modified":1550629419500},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"1dada3c3404445a00367882b8f97cdf092b7943d","modified":1550629419500},{"_id":"themes/next/docs/AUTHORS.md","hash":"51a0a13da55ff3d596970b2f9ab4531c6b2211f2","modified":1550629419501},{"_id":"themes/next/docs/DATA-FILES.md","hash":"9a1895c0a0db705c4c48f512e86917f9af1ec3fb","modified":1550629419501},{"_id":"themes/next/docs/INSTALLATION.md","hash":"b74ef6fedf76cdb156e2265759ee0a789ddd49cc","modified":1550629419502},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"721a1aa9feed1b580ab99af8e69ed22699121e88","modified":1550629419503},{"_id":"themes/next/docs/LICENSE.txt","hash":"ae5ad07e4f4106bad55535dba042221539e6c7f9","modified":1550629419503},{"_id":"themes/next/docs/MATH.md","hash":"0540cd9c961b07931af9f38a83bc9a0f90cd5291","modified":1550629419504},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"c9f2ed8e15c137b1885d9ca8b7197d9f457971e9","modified":1550629419504},{"_id":"themes/next/languages/de.yml","hash":"641e49587d41bb87e4d5932dc3d975754ded7953","modified":1550629419514},{"_id":"themes/next/languages/default.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1550629419514},{"_id":"themes/next/languages/fr.yml","hash":"ebcd1f188af8c3f5ef1f0923e794c839fbfae2d4","modified":1550629419515},{"_id":"themes/next/languages/en.yml","hash":"d66b8b48840443a4f9c72c7696a21e292f685a47","modified":1550629419515},{"_id":"themes/next/languages/id.yml","hash":"9709a4dbacc56a1571a96b139b872128d6959e90","modified":1550629419516},{"_id":"themes/next/languages/it.yml","hash":"4e3adeb10c0fa627935d69ae1783ce0894f5dee5","modified":1550629419516},{"_id":"themes/next/languages/ja.yml","hash":"82afb0a5637ad67065fa5b2624fa56c7c240c3c6","modified":1550629419517},{"_id":"themes/next/languages/ko.yml","hash":"33e065ceb21590b8eb32430a69e76c2f057eb758","modified":1550629419517},{"_id":"themes/next/languages/nl.yml","hash":"060efc260c1c529469d739d97dcee79683e8f411","modified":1550629419518},{"_id":"themes/next/languages/pt-BR.yml","hash":"dc09e290e908744ca28e093dbdd859ca2a20290e","modified":1550629419519},{"_id":"themes/next/languages/pt.yml","hash":"53e2a52b9d5dc20c04080acd4f5b954e8699780f","modified":1550629419519},{"_id":"themes/next/languages/ru.yml","hash":"720b92a9ec075b68737d296b1f29ad8e01151c85","modified":1550629419520},{"_id":"themes/next/languages/tr.yml","hash":"6d2f53d3687a7a46c67c78ab47908accd8812add","modified":1550629419520},{"_id":"themes/next/languages/uk.yml","hash":"6320439c6e9ff81e5b8f8129ca16e9a744b37032","modified":1550629419521},{"_id":"themes/next/languages/vi.yml","hash":"e2b3b18359ab41d58c64b2002acfedd60a7505a4","modified":1550629419521},{"_id":"themes/next/languages/zh-CN.yml","hash":"069f15da910d6f9756be448167c07ea5aa5dc346","modified":1550629419522},{"_id":"themes/next/languages/zh-HK.yml","hash":"c22113c4a6c748c18093dae56da5a9e8c5b963cd","modified":1550629419522},{"_id":"themes/next/languages/zh-TW.yml","hash":"dbf4dd87716babb2db4f5332fae9ec190a6f636a","modified":1550629419523},{"_id":"themes/next/layout/_layout.swig","hash":"00a38a4d25cc0b42c40b7011428d29b0d40b5d88","modified":1550629419525},{"_id":"themes/next/layout/archive.swig","hash":"4b53070008775ecfd03953bd1b4adfcb0fabcaac","modified":1550629419574},{"_id":"themes/next/layout/category.swig","hash":"f0e3338bfa5efb205d2c28e635e9611f1fff3b55","modified":1550629419575},{"_id":"themes/next/layout/index.swig","hash":"bdcc9f57adef49706b16b107791cacecbc23c1dc","modified":1550629419575},{"_id":"themes/next/layout/page.swig","hash":"1d28997ec69d3c236c0dac86cc7a8fbcfdbbb03f","modified":1550629419576},{"_id":"themes/next/layout/post.swig","hash":"af74e97d57cf00cde6f8dbd4364f27910915454e","modified":1550629419576},{"_id":"themes/next/layout/schedule.swig","hash":"e79f43df0e9a6cf48bbf00882de48c5a58080247","modified":1550629419577},{"_id":"themes/next/layout/tag.swig","hash":"7cda2822e50b9fee9848a6b81e6c2d1aca830aeb","modified":1550629419577},{"_id":"themes/next/scripts/merge.js","hash":"39b84b937b2a9608b94e5872349a47200e1800ff","modified":1550629419582},{"_id":"themes/next/scripts/merge-configs.js","hash":"5f96f63e86825fd7028c2522e4111103e261a758","modified":1550629419581},{"_id":"themes/next/test/.jshintrc","hash":"c9fca43ae0d99718e45a6f5ce736a18ba5fc8fb6","modified":1550629419684},{"_id":"themes/next/test/helpers.js","hash":"f25e7f3265eb5a6e1ccbb5e5012fa9bebf134105","modified":1550629419685},{"_id":"themes/next/test/intern.js","hash":"db90b1063356727d72be0d77054fdc32fa882a66","modified":1550629419685},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1550629419649},{"_id":"source/_posts/coding/git-undo-commits.md","hash":"28fd83f0e0dca457564080620e366a6692a26006","modified":1550630660642},{"_id":"source/_posts/coding/exit_after_timeout.ipynb","hash":"05e6a19c89f1d14f07392db66bd944f3b8edada0","modified":1556545343726},{"_id":"source/_posts/statitics/Statistics.md","hash":"0df82b69b6d178f8d4cafcca2b948d80eb11ba7d","modified":1556545343729},{"_id":"source/_posts/statitics/estimate.md","hash":"ae151acb44faac02cd728e6e487dd2942684f3ad","modified":1556545557230},{"_id":"source/_posts/statitics/entropy.md","hash":"e6bea2bceeab517c023a4c4b366f1b431e2c3f3a","modified":1560770781252},{"_id":"source/_posts/xiguashu/decision-tree.md","hash":"8b088897a1e2124526eb96891e52e1d6f71ee6fd","modified":1550629419435},{"_id":"source/_posts/xiguashu/index.md","hash":"3990b702fd0a47bfa830ee7d50f499168fee11ce","modified":1550629419439},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"a51de08657f5946f4028b11373280ddc04639525","modified":1550629419505},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"7b2963daac19b0c14f98ebef375d5fbce8fc3f44","modified":1550629419505},{"_id":"themes/next/docs/ru/README.md","hash":"0b69e831b4617f801d9b49e8ce810daacbbee6c1","modified":1550629419506},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"1a4e41adcf5831057f3f7b3025ed4a5ef7c442b4","modified":1550629419507},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"e771c5b745608c6fb5ae2fa1c06c61b3699627ec","modified":1550629419507},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"6ea741f380dc3e90661d12db7e115a94b77643a4","modified":1550629419509},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"baca12cc24be082f1db28c7f283493569666321c","modified":1550629419510},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"9b512cb820627fcc45c9f77c7a122aa99e021bd5","modified":1550629419508},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"67f4a987e7db0ab1ce1ea4c311f2961df07b6681","modified":1550629419509},{"_id":"themes/next/docs/zh-CN/README.md","hash":"7fb215a0a633384948ac9228ac14617b4a974dbd","modified":1550629419511},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"b17fc344ff61603f83387c0f9b2b2189aae81d50","modified":1550629419510},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"734b371a0dd910eb9fe087f50c95ce35340bb832","modified":1550629419511},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"2095d1214a4e519a1d31b67b41c89080fa3285d3","modified":1550629419512},{"_id":"themes/next/layout/_custom/head.swig","hash":"a223919d2e1bf17ca4d6abb2c86f2efca9883dc1","modified":1550629419524},{"_id":"themes/next/layout/_custom/header.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1550629419524},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1550629419524},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"891ab67815969dd8736cb22fbbb3f791b8fff4e4","modified":1550629419528},{"_id":"themes/next/layout/_macro/post.swig","hash":"799bd11921682c51db6cb92ed57b6a8679c93ee1","modified":1550629419528},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"838686a5d2c3ae5c03b69842e931d9efe276e55a","modified":1550629419529},{"_id":"themes/next/layout/_partials/comments.swig","hash":"54afb7b78509ed8fac5d23daecc147b0fe615d1d","modified":1550629419530},{"_id":"themes/next/layout/_partials/footer.swig","hash":"6d56acdcdc12ebca9c1d90f8a2b52ad17aafca6e","modified":1550629419530},{"_id":"themes/next/layout/_partials/github-banner.swig","hash":"710ae10cb6cafc21e40c57dd9b31f0980b030f4b","modified":1550629419531},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"914155d5d758306cff405beefd4a07973fd8fc77","modified":1550629419537},{"_id":"themes/next/layout/_partials/post-edit.swig","hash":"dee345054d564dd56f74bb143942d3edd1cb8150","modified":1550629419537},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"6337747816219616916e73bdfd6961a164f872cf","modified":1550629419545},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"efb3404a3303622f3be60944d9d1926972c5c248","modified":1550629419545},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"c31d54154eed347f603009d2d65f7bf8d9a6885a","modified":1550629419545},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"68ad21aef6481d014e7ec0b674e469f2f82ea231","modified":1550629419549},{"_id":"themes/next/layout/_third-party/bookmark.swig","hash":"4b93dc7ac0573c402aabcb5c933bbcb893b07c51","modified":1550629419557},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"9e00cb9b3fdfe2e2c4877a874d0d3ecb7fd0f3ee","modified":1550629419563},{"_id":"themes/next/layout/_third-party/copy-code.swig","hash":"c80b04aabdeef0e03a08284c82324089710abd45","modified":1550629419562},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"7db4ad4a8dd5420dad2f6890f5299945df0af970","modified":1550629419567},{"_id":"themes/next/layout/_third-party/pangu.swig","hash":"76f5933925670044ec65b454295ba7e0a8439986","modified":1550629419567},{"_id":"themes/next/layout/_third-party/pdf.swig","hash":"d66dec72ea7ad5026260914e3545551645e0ab37","modified":1550629419568},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"85ca359866325ce82f5f0f88576fae13a763b5eb","modified":1550629419569},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"c476dc3693a9dd0be2d136a45b0d7fdef55d4d92","modified":1550629419569},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"f58463133bf8cfef5ff07f686b834ff8cbbe492f","modified":1550629419569},{"_id":"themes/next/scripts/helpers/next-url.js","hash":"3db90c52bc8af57c6fa4a9dbdc524e1ecdc8b7b1","modified":1550629419580},{"_id":"themes/next/scripts/helpers/engine.js","hash":"cdb6152582313268d970ffeef99b4a8a7850f034","modified":1550629419580},{"_id":"themes/next/scripts/tags/exturl.js","hash":"e9dab948e6327b22b4fc0c66cf97f6600a444dd5","modified":1550629419583},{"_id":"themes/next/scripts/tags/button.js","hash":"dba55534ef4a11fb6723ec73e3ef4dcf7589ed9f","modified":1550629419582},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"4519ab8e6898f2ee90d05cde060375462b937a7d","modified":1550629419583},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"03575b534303f42c81ba6187ec601a5578913a39","modified":1550629419584},{"_id":"themes/next/scripts/tags/full-image.js","hash":"b2ed8de4065c302fac1654f0d3d3ecb5fa6e0f80","modified":1550629419583},{"_id":"themes/next/scripts/tags/include-raw.js","hash":"ab4a82a7246265717556c7a42f897430340b88cf","modified":1550629419585},{"_id":"themes/next/scripts/tags/label.js","hash":"48f68ab33d42e638c2ab5f89f40c34fc2fa1e6dc","modified":1550629419585},{"_id":"themes/next/scripts/tags/note.js","hash":"f1b560d6e63d1b06fd80e12bbac32660125c223c","modified":1550629419586},{"_id":"themes/next/scripts/tags/tabs.js","hash":"ca885c8fa46a76a7b8977730575551622497410b","modified":1550629419586},{"_id":"themes/next/source/css/main.styl","hash":"5e7d28bc539e84f8b03e68df82292f7fc0f2d023","modified":1550629419649},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1550629419650},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1550629419651},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1550629419651},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1550629419652},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1550629419653},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1550629419654},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1550629419654},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1550629419655},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1550629419655},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1550629419656},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1550629419657},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1550629419656},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1550629419657},{"_id":"themes/next/source/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1550629419657},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1550629419658},{"_id":"themes/next/source/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1550629419659},{"_id":"themes/next/source/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1550629419659},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1550629419660},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1550629419631},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1550629419631},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1550629419632},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1550629419647},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1550629419648},{"_id":"source/_posts/xiguashu/decision-tree/1548940093157.png","hash":"1545a8cdd264da9e41929b5b22c98fabb40b14ba","modified":1550629419436},{"_id":"source/_posts/xiguashu/decision-tree/1548940132559.png","hash":"dc050e44987e396207cdc6b32c689b134207f97c","modified":1550629419437},{"_id":"source/_posts/xiguashu/decision-tree/1548940125578.png","hash":"dc050e44987e396207cdc6b32c689b134207f97c","modified":1550629419437},{"_id":"source/_posts/xiguashu/decision-tree/feature-space.png","hash":"1341d484f40cb56ae4c2295bfe49fb3843db815e","modified":1550629419438},{"_id":"source/_posts/xiguashu/decision-tree/tree_basic.png","hash":"63a086ff52e9fc6b2d59470db972e23737e30369","modified":1550629419439},{"_id":"themes/next/layout/_macro/menu/menu-badge.swig","hash":"4eb8e222dc337211efb0d3bbdb5e29af3e6ecdb8","modified":1550629419526},{"_id":"themes/next/layout/_macro/menu/menu-item.swig","hash":"25aea3d764b952f3f6d28ab86d7212d138e892df","modified":1550629419527},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"b57bf9c865bed0f22157176a8085de168a1aef77","modified":1550629419532},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"4b53a0659a7e800871d8e9a4bd20f7b892a8e29b","modified":1550629419532},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"187316a1d565f98eac138b5ce2180b52a190028c","modified":1550629419533},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"2177da1d3c64abb6b3398f90885cf543ae6fe6da","modified":1550629419533},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"c909f6e96373c151dea325bcddfdd8c9522421b6","modified":1550629419534},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"e015c7d9b84062b60b15b36be3ef11929dd10943","modified":1550629419535},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"e92154bf3d46127aa51858734da3a295944cf883","modified":1550629419535},{"_id":"themes/next/layout/_partials/page/breadcrumb.swig","hash":"daa6e5b7dbc409d6bf8a031d5413d8229e9c0995","modified":1550629419536},{"_id":"themes/next/layout/_partials/page/page-header.swig","hash":"f46699a9daa5fef599733cbab35cb75cf7a05444","modified":1550629419536},{"_id":"themes/next/layout/_partials/post/post-copyright.swig","hash":"55de88525c05af6053524c78ef4b42b46c3bfb6d","modified":1550629419538},{"_id":"themes/next/layout/_partials/post/post-related.swig","hash":"f331ad02beea8990066d32ad6ec9f859672c3615","modified":1550629419539},{"_id":"themes/next/layout/_partials/post/reward.swig","hash":"b6e7d0af24b4d52d168a4eb0fb29bffa69621395","modified":1550629419539},{"_id":"themes/next/layout/_partials/post/wechat-subscriber.swig","hash":"0949c72a944b9abd5e5cc654b5556ac9450d431b","modified":1550629419540},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"f6454c452b2e90a8c760321bce7e3dc6119b71fa","modified":1550629419541},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"b2f0d247b213e4cf8de47af6a304d98070cc7256","modified":1550629419541},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"31245e09ce0465b994cebd94223a531585c4eab4","modified":1550629419542},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"b25002a83cbd2ca0c4a5df87ad5bff26477c0457","modified":1550629419542},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"9e3d133ac5bcc6cb51702c83b2611a49811abad1","modified":1550629419543},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"0110cdf5233de4d26ccddd1e84497b0fd18d16fa","modified":1550629419544},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"54b43d406cf37932e7b60f46814e864d31b1842c","modified":1550629419543},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"e0f0a753d4920ffb37ddbc8270515654a0b9b92a","modified":1550629419546},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a62c93f19429f159bcf0c2e533ffc619aa399755","modified":1550629419547},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"3c548934b97cc426544947f7a2ae35c270b5e33f","modified":1550629419548},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"84018384d00e4a584d613589adae6674a3060a36","modified":1550629419548},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"23c6d15aa2a305f9d29caee1b60cfae84d32fa09","modified":1550629419549},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"66d562b3778dbc839f7c00103bd0099c5d61602a","modified":1550629419550},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"83dd7df11b100bae38c9faab9a478f92149a0315","modified":1550629419551},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"73576c9683d9ad9b124916dc6c660607fe7cc1fa","modified":1550629419551},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"2e1de38f44af00209129d4051b7ae307cb11ad68","modified":1550629419552},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"335005a9f8b36349f0ad0a7beeba6969c55fc7f7","modified":1550629419552},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"53202062267391353d49f269e7eb74eb87d30921","modified":1550629419553},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"8ab040fccba41675bc835973515530af8a51f8bd","modified":1550629419553},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"69d7697cbf423efad54d47dad038a5afc2e02695","modified":1550629419554},{"_id":"themes/next/layout/_third-party/analytics/growingio.swig","hash":"42a2f9e708100d63dac55bb8e1ca5f024e4e1162","modified":1550629419554},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"39928f358dd13d9fc1a4641800e57be157ecd815","modified":1550629419555},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"a4d752d17ddfc579730401ff3e3dfd4ec290c8de","modified":1550629419555},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"d18c87d7839e7407e39acd2998bcc9e0b34611b0","modified":1550629419556},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"a22d1ea29a5ffe46199ab7d108a291a05af8d5b6","modified":1550629419556},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"6143aa0ed0d9bbe24c5859f66ea0a5611d73ecd2","modified":1550629419557},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"bc3fc9d053b3d1fc0cd3918bf9a629a6f38f6414","modified":1550629419558},{"_id":"themes/next/layout/_third-party/comments/gitalk.swig","hash":"8bcc3430241e267a06abfbb4e9ca125838bc5a71","modified":1550629419559},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"d685df1516cb138d7a83bac5d7878a1e0fa8bc04","modified":1550629419558},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"0149e803ed7d30163df3b3ba3f578e5584a2e4af","modified":1550629419560},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"bd529bf26f28745eb8ded3be7652d33d55fec8d4","modified":1550629419561},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"1a5d94f5779a2ce13abc886dd78e0617f89c34b9","modified":1550629419561},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"bd6d763c1233cd89512c323ce0992daf7f9fd0e5","modified":1550629419562},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"43a20fa0e9ae2f4254f04813f9c619dd36b49ae5","modified":1550629419564},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"ea1c136f960667a0a13b334db497b9b19c41f629","modified":1550629419565},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"6f7217ff64cf67ee239954d3499a3c9e76bad061","modified":1550629419566},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"34599633658f3b0ffb487728b7766e1c7b551f5a","modified":1550629419571},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"d45ca53af17d1d83fd27f8ed0917a72f0060e1a9","modified":1550629419572},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"fe95dd3d166634c466e19aa756e65ad6e8254d3e","modified":1550629419573},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"ce101432cca9cc174730eb75ca65185b2cb2e456","modified":1550629419574},{"_id":"themes/next/scripts/filters/after_post_render/exturl.js","hash":"bfa8e5a91d22a5ba22e8f3da337fdb5e8b2600fd","modified":1550629419579},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"3403fdd8efde1a0afd11ae8a5a97673f5903087f","modified":1550629419630},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"07f7da320689f828f6e36a6123807964a45157a0","modified":1550629419631},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"8aa98ae349908736ba43196c42498fd5bdeb780a","modified":1550629419632},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"d1deb849e697cfb6258b8ab7bfb47e219210ccd9","modified":1550629419632},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"e9b0752f08398709e787546a246baca12b4c557f","modified":1550629419646},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"a25408534f8fe6e321db4bbf9dd03335d648fe17","modified":1550629419647},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"da7049f3d9a157abe0ecc62611edcf43605ba84d","modified":1550629419648},{"_id":"themes/next/source/css/_variables/base.styl","hash":"e37aab667be94576f6145b61a78cfe87836c68b6","modified":1550629419648},{"_id":"themes/next/source/js/src/affix.js","hash":"ad343aa406fd8181b5f310434817ce98fc2219e3","modified":1550629419661},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"84906eeae57bd06744dd20160b93eacf658f97e2","modified":1550629419661},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"c7e2a588b679d46379124141bb2f30bc2f3210e2","modified":1550629419662},{"_id":"themes/next/source/js/src/exturl.js","hash":"c48aa4b3c0e578a807fd3661e6cd4f3890777437","modified":1550629419662},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"1512c751d219577d338ac0780fb2bbd9075d5298","modified":1550629419663},{"_id":"themes/next/source/js/src/motion.js","hash":"e70f961d24e4e61a2df5bf640ab51acee8f1ffbd","modified":1550629419663},{"_id":"themes/next/source/js/src/post-details.js","hash":"7d309b771e86c7e22ce11cc25625481ef7d5985c","modified":1550629419664},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"c4867626afab749404daf321367f9b6b8e223f69","modified":1550629419665},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"68d3690152c89e7adb08bb35ec28dbda2bd93686","modified":1550629419666},{"_id":"themes/next/source/js/src/utils.js","hash":"e1dc6941b3c545dd0c1f71b49ac277673dfca61a","modified":1550629419666},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"b4aefc910578d76b267e86dfffdd5121c8db9aec","modified":1550629419667},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"03ddbf76c1dd1afb93eed0b670d2eee747472ef1","modified":1550629419668},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"c31ff06a740955e44edd4403902e653ccabfd4db","modified":1550629419669},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1550629419669},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"71e7183634dc1b9449f590f15ebd7201add22ca7","modified":1550629419669},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1550629419682},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"dbbfb50f6502f6b81dcc9fee7b31f1e812da3464","modified":1550629419682},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1550629419683},{"_id":"source/_posts/coding/images/1553235348583.png","hash":"39d5d01e6dca9aa5a6e99a32eef64ed208b0f5c5","modified":1556545343728},{"_id":"themes/next/source/lib/jquery/index.js","hash":"17a740d68a1c330876c198b6a4d9319f379f3af2","modified":1550629419677},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"89e41d4c298d8d70b4d1c833c7e599d089f2b3d4","modified":1550629419570},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"2530de0f3125a912756f6c0e9090cd012134a4c5","modified":1550629419571},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"1a4ac0d119f2126ef8951897338706edce112235","modified":1550629419626},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"bbb788b453236e5b4af7c81df8efcfc6bde08903","modified":1550629419587},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"237d185ac62ec9877e300947fa0109c44fb8db19","modified":1550629419588},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"8b32928686c327151e13d3ab100157f9a03cd59f","modified":1550629419589},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"ff4489cd582f518bba6909a301ac1292a38b4e96","modified":1550629419589},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"7ad4081466b397e2a6204141bb7768b7c01bd93c","modified":1550629419590},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"4f2801fc4cf3f31bf2069f41db8c6ce0e3da9e39","modified":1550629419599},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"c97c819a65f6967485184399397601e5133deda6","modified":1550629419617},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"c0e8647244e1ef106e94c3c8ac4a64bca2677159","modified":1550629419627},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"cb2c0beb69bfc56c0ed86e609bc1c35edb799b99","modified":1550629419627},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"33a74fcd6c38cea356f6d2994a19f46dcfd5d8a4","modified":1550629419628},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a17e2b871a335f290afb392a08f94fd35f59c715","modified":1550629419629},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"3f40e8a9fe8e7bd5cfc4cf4cbbbcb9539462e973","modified":1550629419629},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"fc491fd2d53d338f63e1be7d3ed312f8065c7b93","modified":1550629419630},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"5f1e5d7b9d44ef3e6ad442e083753fee76d3b112","modified":1550629419633},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"d0e9065b0dbbc01811259f0597d1790268b4881b","modified":1550629419634},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"eecb2081ee1eef1e2152c7fea9310366e33b1eac","modified":1550629419634},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"b1025c421406d2c24cc92a02ae28c1915b01e240","modified":1550629419635},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"a80782a6eb3b40da2e14251da49069aff3115d8a","modified":1550629419635},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"06d9d00257abd28414ec0b746f866bf9911cf5ec","modified":1550629419636},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"9f35b95beb344f4eeca5ca584fbe7206f791372e","modified":1550629419636},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"15254414a52f05618c54a2ac7f4635f99077ec30","modified":1550629419636},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"4ed12bf17eeb7cd4f22dd01fdd486cda68d169a8","modified":1550629419640},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"31127dcbf4c7b4ada53ffbf1638b5fe325b7cbc0","modified":1550629419639},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"9f35b95beb344f4eeca5ca584fbe7206f791372e","modified":1550629419640},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"157e6915dcf5990566e463acffa71043b2651c07","modified":1550629419639},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"b26f8a3394d8357a5bfd24d9f8bf62d7b4063ebb","modified":1550629419641},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"30d61fa31e405fcfe3d2ff6174ccad60be1745f9","modified":1550629419642},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"28f0444ccdc85a34ada651d8ee52479e16311167","modified":1550629419643},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"1f6e2ce674735269599acc6d77b3ea18d31967fc","modified":1550629419644},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"a5395766dfdda81285d0cd3ddebe8e8bc924fa2a","modified":1550629419645},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"11fcaaf7524445a194801e1048ea2fb84b316414","modified":1550629419643},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"0d6f0df798449b710e1e5dbd43d470089b2a3c95","modified":1550629419645},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"a558803ca81cceae2bdc22c18ef638fcc023681b","modified":1550629419645},{"_id":"themes/next/source/js/src/schemes/muse.js","hash":"ccc0c5cd4ec6f8159c98990ad83f11a5c0b0234c","modified":1550629419664},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"3eea56cc9ce47bb4760930c4c69cebf847a7fbb2","modified":1550629419665},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"3655f1fdf1e584c4d8e8d39026093ca306a5a341","modified":1550629419670},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1550629419671},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"88af80502c44cd52ca81ffe7dc7276b7eccb06cf","modified":1550629419672},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"41ea797c68dbcff2f6fb3aba1d1043a22e7cc0f6","modified":1550629419678},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"a817b6c158cbc5bab3582713de9fe18a18a80552","modified":1550629419679},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1550629419675},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1550629419676},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"4237c6e9d59da349639de20e559e87c2c0218cfd","modified":1550629419681},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"2df409df04fdb52d7234876a9f6e502edd4e3929","modified":1550629419590},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"1ec3102ee8f5b8cc0877da1fd109d37470401e7b","modified":1550629419593},{"_id":"themes/next/source/css/_common/components/header/github-banner.styl","hash":"48bb741f6bda73b322a25a8fbe37fd3d5e0ff601","modified":1550629419591},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"34f5ac3c1ed2dd31e9297cc4c0733e71bc2e252f","modified":1550629419592},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d63e0cacc53dd375fcc113465a4328c59ff5f2c1","modified":1550629419592},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"b8647d6140141b0a160607f6353e4d4594cca92e","modified":1550629419593},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"1727702eac5d326b5c81a667944a245016668231","modified":1550629419594},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"32bbf059c57677e754a918c927ac63e2d843108f","modified":1550629419595},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"167986d0f649516671ddf7193eebba7b421cd115","modified":1550629419594},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"7fe4d4d656e86276c17cb4e48a560cb6a4def703","modified":1550629419595},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"e5a5f8747fdf2ca960e4e73c081b8952afd62224","modified":1550629419596},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"fa1cea6fcc3f552d57cc7d28380a304859139bf6","modified":1550629419596},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"4e3838d7ac81d9ad133960f0f7ed58a44a015285","modified":1550629419598},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"7fb593f90d74a99c21840679933b9ef6fdc16a61","modified":1550629419597},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"ad4cae23c8e383f4fabc9a2a95bca6055020d22e","modified":1550629419597},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"db1df0186a4572844d69d0d7bb974bd120cb64d5","modified":1550629419599},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"62fbbd32cf5a99ae550c45c763a2c4813a138d01","modified":1550629419600},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"77da38898bdd99cf8fd3e0ae8cc4d2ac943bcb60","modified":1550629419605},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"fc94dd09b4245143b452d6cf2fc4c12134d99d6d","modified":1550629419603},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"3241c9ae85ca11b6c4e125ac471aa4342ba1ce9c","modified":1550629419601},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"8478b51892b663f39f7eb830476315e804cbf037","modified":1550629419605},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"cd9e214e502697f2f2db84eb721bac57a49b0fce","modified":1550629419606},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"330c8884efb5612e7eb03986d87d29e8b0651974","modified":1550629419606},{"_id":"themes/next/source/css/_common/components/post/post-reading_progress.styl","hash":"2ea91d7b75966d471bf857a9f3fbf87fd01aea90","modified":1550629419607},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"989b7d718914b5242506947aa5767b3f2480d8f9","modified":1550629419607},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"7b69c1ad392f8a386854e318d4c8ddeb9ba8d793","modified":1550629419607},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"b2495ae5e04dcca610aacadc47881d9e716cd440","modified":1550629419608},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5a982d8ef3b3623ea5f59e63728990f5623c1b57","modified":1550629419609},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"539fc0880b2e035e8316d5d4b423703195c1b7ba","modified":1550629419609},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"981795aad232c8bd3f52a0ed8720db696d18a234","modified":1550629419610},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"2e36956c5f8802f703c7ce3893d16323a0e09d4c","modified":1550629419611},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"01567edaea6978628aa5521a122a85434c418bfd","modified":1550629419610},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"fa57ec9a6f1943c0558856dfba2d6b8faca0cd4d","modified":1550629419611},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"2e26e4429c2457b8ca12555426659c2fc65a5cea","modified":1550629419612},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"41858f2dfd34a57d0a2016b1bce08efb61943b7a","modified":1550629419613},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"4c1705e5c3fbf0e3f5d3fb29edec4a29c87b4c95","modified":1550629419614},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"11c22f0fb3f6beb13e5a425ec064a4ff974c13b7","modified":1550629419613},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"dd5f7057680faefc9306fab3172106f762c1a517","modified":1550629419615},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"7e2ba73073daaea0a18c3d67ff137dd683af7011","modified":1550629419614},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"b9a19654b6a2685b5426afac8b09cdbd80fae00c","modified":1550629419615},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"4ec203c52db47bca67c229ef1f3693c9d40b4ef9","modified":1550629419616},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"f1fdfd0bc6b66a38581a4fbe096b25aadfc53246","modified":1550629419616},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"2fe76476432b31993338cb45cdb3b29a518b6379","modified":1550629419617},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"2d58ad90f148e845bc7023751a7a13260600f8d6","modified":1550629419618},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"2ad1a2a9bbf6742d1b0762c4c623b68113d1e0fe","modified":1550629419618},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"72cabf6edfd64697e37950cc3e66fbea6ba47b66","modified":1550629419619},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"2ab1322fe52ab5aafd49e68f5bd890e8380ee927","modified":1550629419619},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"2937e566ad0f3d9adc0865b269fdca62b7576fdd","modified":1550629419620},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"30ccc107061dc23943198f087759079161ee24e9","modified":1550629419620},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"f1640253cbbf71d0c04c34c25bd61045894f98bc","modified":1550629419621},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"f26c32a0c3045e5ae826b983abc3a3c139456663","modified":1550629419622},{"_id":"themes/next/source/css/_common/components/third-party/gitalk.styl","hash":"a01484e350ad5fc9b1fdfbfafb2ddd9687ad4d20","modified":1550629419622},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"2a1008f1044b450b806adc166754ba9513e68375","modified":1550629419623},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"49913bb2b702d52d77528cd9378126aa67c0082c","modified":1550629419623},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"74412b0bf4ec0d28aedd2e60b27affd4d5cd1452","modified":1550629419625},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"16087276945fa038f199692e3eabb1c52b8ea633","modified":1550629419624},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"3529aae283864fa0f09925ee8217905632e9a930","modified":1550629419625},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"28825ae15fa20ae3942cdaa7bcc1f3523ce59acc","modified":1550629419624},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"a07aa12cc36ac5c819670c2a3c17d07ed7a08986","modified":1550629419637},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1550629419638},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1550629419642},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1550629419674},{"_id":"public/leancloud_counter_security_urls.json","hash":"e1408683e7e2bb0940f8858156852a0f4c572666","modified":1560782358586},{"_id":"public/categories/index.html","hash":"789c0e47038e34b121b6c35d99cb2f699bb0b69f","modified":1560782358753},{"_id":"public/tags/index.html","hash":"585abba013250d5596924450844b68f78822d905","modified":1560782358753},{"_id":"public/2019/04/29/statitics/estimate/index.html","hash":"6563a4ed9c5ae240236daacc5a469a122b905807","modified":1560782358754},{"_id":"public/2019/03/23/Untitled/index.html","hash":"c404745a75b2424a4545c7a12480e9b30e9ca445","modified":1560782358754},{"_id":"public/2019/01/31/xiguashu/index/index.html","hash":"e5350d6316877c8fd027e0b75e57688db8e0cbf4","modified":1556545838840},{"_id":"public/archives/index.html","hash":"afefc0cb14f0a4bfe8ecee7013f4b0f8be47f154","modified":1560782358754},{"_id":"public/archives/2019/index.html","hash":"78249c212e96c7f18d80da23f534c0bcbff89b1e","modified":1560782358754},{"_id":"public/archives/2019/02/index.html","hash":"5880aac7b0743956a86442c384178a259a2900df","modified":1556545838840},{"_id":"public/archives/2019/01/index.html","hash":"e535c8acca93a2c53b0cecc5ebf63d63d5682b22","modified":1560782358754},{"_id":"public/archives/2019/03/index.html","hash":"fff596466a13dfd6d1e3708e465830ca06b26ee9","modified":1560782358754},{"_id":"public/archives/2019/04/index.html","hash":"519087e8e28d43487d80d3136d844fe08d7ac64f","modified":1560782358754},{"_id":"public/categories/coding/index.html","hash":"89226ce4aa3ffe33073c46bda0eab390c142db5b","modified":1556545838841},{"_id":"public/categories/xiguashu/index.html","hash":"0322476faa72fb8ab5ccd0e196159b29cf9edcc1","modified":1560782140993},{"_id":"public/categories/math/index.html","hash":"ba75a883c4689d9b25a24a86e175ae861d789809","modified":1560782358754},{"_id":"public/tags/git/index.html","hash":"9a9ac92c6db3a9527ee3043ce907d02fca5ea224","modified":1556545838841},{"_id":"public/tags/statistics/index.html","hash":"8611f0a0d071677856872d33c32ecf90598a5efb","modified":1560782358754},{"_id":"public/tags/xiguashu/index.html","hash":"e96f414d06da1eab73ea02ceb5e97bf4df66c291","modified":1560782140994},{"_id":"public/tags/hexo-asset-image/index.html","hash":"35543212e3776eba9bc2d1f5f22f7555c2002b78","modified":1560782358755},{"_id":"public/2019/03/25/statitics/entropy/index.html","hash":"8c0ead2744087a6a809393fdfba5dabd24c33870","modified":1560782358755},{"_id":"public/2019/02/20/coding/git-undo-commits/index.html","hash":"66ea5f573c89dc3049ce0a5033cca28f984365e4","modified":1556545838841},{"_id":"public/2019/01/31/xiguashu/decision-tree/index.html","hash":"e36065a6eb1d9ca68afd710ccc260c1068e1ff1c","modified":1556545838841},{"_id":"public/2019/01/06/statitics/Statistics/index.html","hash":"322630662caf5141ac410e978d8ac68f68d2699e","modified":1560782358755},{"_id":"public/index.html","hash":"4833f79d00f5db175fa0545103294429249f1801","modified":1560782358755},{"_id":"public/tags/math/index.html","hash":"96f3c570decee40adb8928e358e0ccc7b32ac72e","modified":1560782358755},{"_id":"public/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1560782141009},{"_id":"public/CNAME.bak","hash":"cf67bbbfbd1ce091afeba3a3208c7f218826df71","modified":1560782141009},{"_id":"public/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1560782141010},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1560782141009},{"_id":"public/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1560782141010},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1560782141010},{"_id":"public/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1560782141009},{"_id":"public/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1560782141010},{"_id":"public/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1560782141009},{"_id":"public/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1560782141010},{"_id":"public/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1560782141009},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1560782141010},{"_id":"public/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1560782141010},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1560782141011},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1560782141011},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1560782141011},{"_id":"public/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1560782141011},{"_id":"public/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1560782141011},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1560782141011},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1560782141011},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1560782141011},{"_id":"public/2019/01/31/xiguashu/decision-tree/1548940093157.png","hash":"1545a8cdd264da9e41929b5b22c98fabb40b14ba","modified":1556545838851},{"_id":"public/2019/01/31/xiguashu/decision-tree/1548940132559.png","hash":"dc050e44987e396207cdc6b32c689b134207f97c","modified":1556545838851},{"_id":"public/2019/01/31/xiguashu/decision-tree/1548940125578.png","hash":"dc050e44987e396207cdc6b32c689b134207f97c","modified":1556545838851},{"_id":"public/2019/01/31/xiguashu/decision-tree/tree_basic.png","hash":"63a086ff52e9fc6b2d59470db972e23737e30369","modified":1556545838851},{"_id":"public/2019/01/31/xiguashu/decision-tree/feature-space.png","hash":"1341d484f40cb56ae4c2295bfe49fb3843db815e","modified":1556545838851},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1560782141812},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1560782141817},{"_id":"public/js/src/affix.js","hash":"a2aab233d99297435a5274bf512c3c753fe08e80","modified":1560782141831},{"_id":"public/js/src/algolia-search.js","hash":"1f7f10c579e7703d0f6acb8b73f3d78a07d0c623","modified":1560782141831},{"_id":"public/js/src/bootstrap.js","hash":"1c41508b83cb0c4512e64b4d63afa1be954ce8ef","modified":1560782141831},{"_id":"public/js/src/motion.js","hash":"7933a30382a84b655238f6e78d42ea1b99af4de6","modified":1560782141831},{"_id":"public/js/src/exturl.js","hash":"54825acc8de4793feac415be227b965428f4e97d","modified":1560782141831},{"_id":"public/js/src/post-details.js","hash":"0dde5e6d4547587662a3256317a9d5d1db507692","modified":1560782141831},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1560782141831},{"_id":"public/js/src/scrollspy.js","hash":"fa3c92968bcdbcb8d95a1729f7659d9753cbd077","modified":1560782141831},{"_id":"public/js/src/scroll-cookie.js","hash":"d07b3776708d4ae79ed2037c4c7391d5c9b06b19","modified":1560782141832},{"_id":"public/js/src/utils.js","hash":"f1394d64977439ec569d2777b1ac304905e043f1","modified":1560782141832},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1560782141832},{"_id":"public/js/src/schemes/pisces.js","hash":"9eb63cba0327d3d11b6cbfcbe40b88e97a8378a3","modified":1560782141832},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1560782141832},{"_id":"public/js/src/schemes/muse.js","hash":"e9bfa6b343b67625f58757efce46ccdaac8f308c","modified":1560782141832},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1560782141833},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1560782141832},{"_id":"public/css/main.css","hash":"f3e80147a5d675e79dd51d9e4d7459d5d933add4","modified":1560782141833},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1560782141833},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1560782141833},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1560782141833},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1560782141833},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1560782141833},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1560782141834},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1560782141834},{"_id":"source/_posts/deep-learning/nn_cs231n_note.md","hash":"74ea000f97b299c1a77d78de37832c9060cdd888","modified":1560782348652},{"_id":"source/_posts/machine-learning/decision-tree.md","hash":"f1d4d773169696dabf7516d8563ca0f0ddfa52eb","modified":1560782206860},{"_id":"source/_posts/machine-learning/index.md","hash":"3990b702fd0a47bfa830ee7d50f499168fee11ce","modified":1550629419439},{"_id":"source/_posts/deep-learning/images/1560776135809.png","hash":"04f41de9ecec7e34742f0d6b9a1e370184a24b71","modified":1560776135825},{"_id":"source/_posts/deep-learning/images/1560776449074.png","hash":"7e91eee97de8151660f88d4058a8abefdef48781","modified":1560776449088},{"_id":"source/_posts/machine-learning/decision-tree/1548940093157.png","hash":"1545a8cdd264da9e41929b5b22c98fabb40b14ba","modified":1550629419436},{"_id":"source/_posts/machine-learning/decision-tree/1548940125578.png","hash":"dc050e44987e396207cdc6b32c689b134207f97c","modified":1550629419437},{"_id":"source/_posts/machine-learning/decision-tree/1548940132559.png","hash":"dc050e44987e396207cdc6b32c689b134207f97c","modified":1550629419437},{"_id":"source/_posts/machine-learning/decision-tree/tree_basic.png","hash":"63a086ff52e9fc6b2d59470db972e23737e30369","modified":1550629419439},{"_id":"source/_posts/machine-learning/decision-tree/feature-space.png","hash":"1341d484f40cb56ae4c2295bfe49fb3843db815e","modified":1550629419438},{"_id":"source/_posts/deep-learning/images/1560779540116.png","hash":"e75abb556987946287bf853feafe5f554aef81c4","modified":1560779540135},{"_id":"source/_posts/deep-learning/images/42.png","hash":"676fc46d06ed87d6597f2dc71eed1d084824ed9e","modified":1560779150387},{"_id":"source/_posts/deep-learning/images/aae11de6e6a29f50d46b9ea106fbb02a_hd.png","hash":"b5dcff7a36e9d96bb915e0bcefa9efe4a1785315","modified":1560776736256},{"_id":"source/_posts/deep-learning/images/e743b6777775b1671c3b5503d7afbbc4_hd.png","hash":"1aae3b4cde09f9710d2e31aebc20a249b7e9180c","modified":1560776553359},{"_id":"source/_posts/deep-learning/images/d0cbce2f2654b8e70fe201fec2982c7d_hd.png","hash":"3d8190e901a235e3ae38f72dce30604e1e740fa8","modified":1560770955108},{"_id":"source/_posts/deep-learning/images/1560779531173.png","hash":"e83832f9930164a085f6bc9e0b9814785e13d36c","modified":1560779531219},{"_id":"source/_posts/deep-learning/images/63fcf4cc655cb04f21a37e86aca333cf_hd.png","hash":"4aa257d2eb9b46c6812525dec602cacda713fa6f","modified":1560778467646},{"_id":"source/_posts/deep-learning/images/ccb56c1fb267bc632d6d88459eb14ace_hd.png","hash":"6b7ed015213b55d3f41426379ef583e4e4966c65","modified":1560774585366},{"_id":"source/_posts/deep-learning/images/8608c06086fc196228f4dda78499a2d9_hd.png","hash":"361f334962bfb8f19f0f0fc13bca6d0cbb1a3671","modified":1560776754640},{"_id":"source/_posts/deep-learning/images/3.1.1.5.png","hash":"942fd53dc0773b63a7245250ca5b5c44edaf7053","modified":1560772640501},{"_id":"public/2019/01/31/machine-learning/index/index.html","hash":"598f10db2efbb504edf7a170ff7a064d85aa2d31","modified":1560782140992},{"_id":"public/archives/2019/06/index.html","hash":"ed8edf2a1287b510f9f3e5ba7ad1d60cc57bf19d","modified":1560782358754},{"_id":"public/categories/dl/index.html","hash":"2d995bd0d01d57ba36506ae841b8df553fe747ab","modified":1560782358754},{"_id":"public/tags/dl/index.html","hash":"b566a51cde23bcca4cd6ae3271fe5a7807ff85a9","modified":1560782358755},{"_id":"public/2019/06/17/deep-learning/nn_cs231n_note/index.html","hash":"880b8cb532850e81e7c3e6af2d16f1a5af512f2a","modified":1560782358755},{"_id":"public/2019/01/31/machine-learning/decision-tree/index.html","hash":"7798ee9e2357500f917499ac97b4ad4b338d9913","modified":1560782358755},{"_id":"public/2019/01/31/machine-learning/decision-tree/1548940093157.png","hash":"1545a8cdd264da9e41929b5b22c98fabb40b14ba","modified":1560782141011},{"_id":"public/2019/01/31/machine-learning/decision-tree/1548940125578.png","hash":"dc050e44987e396207cdc6b32c689b134207f97c","modified":1560782141011},{"_id":"public/2019/01/31/machine-learning/decision-tree/1548940132559.png","hash":"dc050e44987e396207cdc6b32c689b134207f97c","modified":1560782141011},{"_id":"public/2019/01/31/machine-learning/decision-tree/feature-space.png","hash":"1341d484f40cb56ae4c2295bfe49fb3843db815e","modified":1560782141011},{"_id":"public/2019/01/31/machine-learning/decision-tree/tree_basic.png","hash":"63a086ff52e9fc6b2d59470db972e23737e30369","modified":1560782141011},{"_id":"source/_posts/deep-learning/nn_cs231n_note/1560776135809.png","hash":"04f41de9ecec7e34742f0d6b9a1e370184a24b71","modified":1560776135825},{"_id":"source/_posts/deep-learning/nn_cs231n_note/1560776449074.png","hash":"7e91eee97de8151660f88d4058a8abefdef48781","modified":1560776449088},{"_id":"source/_posts/deep-learning/nn_cs231n_note/1560779540116.png","hash":"e75abb556987946287bf853feafe5f554aef81c4","modified":1560779540135},{"_id":"source/_posts/deep-learning/nn_cs231n_note/42.png","hash":"676fc46d06ed87d6597f2dc71eed1d084824ed9e","modified":1560779150387},{"_id":"source/_posts/deep-learning/nn_cs231n_note/aae11de6e6a29f50d46b9ea106fbb02a_hd.png","hash":"b5dcff7a36e9d96bb915e0bcefa9efe4a1785315","modified":1560776736256},{"_id":"source/_posts/deep-learning/nn_cs231n_note/d0cbce2f2654b8e70fe201fec2982c7d_hd.png","hash":"3d8190e901a235e3ae38f72dce30604e1e740fa8","modified":1560770955108},{"_id":"source/_posts/deep-learning/nn_cs231n_note/e743b6777775b1671c3b5503d7afbbc4_hd.png","hash":"1aae3b4cde09f9710d2e31aebc20a249b7e9180c","modified":1560776553359},{"_id":"source/_posts/deep-learning/nn_cs231n_note/1560779531173.png","hash":"e83832f9930164a085f6bc9e0b9814785e13d36c","modified":1560779531219},{"_id":"source/_posts/deep-learning/nn_cs231n_note/63fcf4cc655cb04f21a37e86aca333cf_hd.png","hash":"4aa257d2eb9b46c6812525dec602cacda713fa6f","modified":1560778467646},{"_id":"source/_posts/deep-learning/nn_cs231n_note/ccb56c1fb267bc632d6d88459eb14ace_hd.png","hash":"6b7ed015213b55d3f41426379ef583e4e4966c65","modified":1560774585366},{"_id":"source/_posts/deep-learning/nn_cs231n_note/8608c06086fc196228f4dda78499a2d9_hd.png","hash":"361f334962bfb8f19f0f0fc13bca6d0cbb1a3671","modified":1560776754640},{"_id":"source/_posts/deep-learning/nn_cs231n_note/3.1.1.5.png","hash":"942fd53dc0773b63a7245250ca5b5c44edaf7053","modified":1560772640501},{"_id":"public/categories/ml/index.html","hash":"e9a9387838310403d1c6356cce1fe4af537a7d06","modified":1560782358761},{"_id":"public/tags/ml/index.html","hash":"adeb7942a42afa2644a52b7c52234c735f4b0c76","modified":1560782358761},{"_id":"public/2019/06/17/deep-learning/nn_cs231n_note/1560776135809.png","hash":"04f41de9ecec7e34742f0d6b9a1e370184a24b71","modified":1560782358764},{"_id":"public/2019/06/17/deep-learning/nn_cs231n_note/1560776449074.png","hash":"7e91eee97de8151660f88d4058a8abefdef48781","modified":1560782358764},{"_id":"public/2019/06/17/deep-learning/nn_cs231n_note/1560779540116.png","hash":"e75abb556987946287bf853feafe5f554aef81c4","modified":1560782358766},{"_id":"public/2019/06/17/deep-learning/nn_cs231n_note/42.png","hash":"676fc46d06ed87d6597f2dc71eed1d084824ed9e","modified":1560782358766},{"_id":"public/2019/06/17/deep-learning/nn_cs231n_note/d0cbce2f2654b8e70fe201fec2982c7d_hd.png","hash":"3d8190e901a235e3ae38f72dce30604e1e740fa8","modified":1560782358767},{"_id":"public/2019/06/17/deep-learning/nn_cs231n_note/aae11de6e6a29f50d46b9ea106fbb02a_hd.png","hash":"b5dcff7a36e9d96bb915e0bcefa9efe4a1785315","modified":1560782358767},{"_id":"public/2019/06/17/deep-learning/nn_cs231n_note/e743b6777775b1671c3b5503d7afbbc4_hd.png","hash":"1aae3b4cde09f9710d2e31aebc20a249b7e9180c","modified":1560782358767},{"_id":"public/2019/06/17/deep-learning/nn_cs231n_note/1560779531173.png","hash":"e83832f9930164a085f6bc9e0b9814785e13d36c","modified":1560782358770},{"_id":"public/2019/06/17/deep-learning/nn_cs231n_note/ccb56c1fb267bc632d6d88459eb14ace_hd.png","hash":"6b7ed015213b55d3f41426379ef583e4e4966c65","modified":1560782358770},{"_id":"public/2019/06/17/deep-learning/nn_cs231n_note/63fcf4cc655cb04f21a37e86aca333cf_hd.png","hash":"4aa257d2eb9b46c6812525dec602cacda713fa6f","modified":1560782358770},{"_id":"public/2019/06/17/deep-learning/nn_cs231n_note/8608c06086fc196228f4dda78499a2d9_hd.png","hash":"361f334962bfb8f19f0f0fc13bca6d0cbb1a3671","modified":1560782358774},{"_id":"public/2019/06/17/deep-learning/nn_cs231n_note/3.1.1.5.png","hash":"942fd53dc0773b63a7245250ca5b5c44edaf7053","modified":1560782358780}],"Category":[{"name":"coding","_id":"cjv2f7asc0006jwfy0h04df6n"},{"name":"xiguashu","_id":"cjv2f7ash000bjwfy07lvsbos"},{"name":"math","_id":"cjv2f7asi000fjwfyvpn8rsh2"},{"name":"dl","_id":"cjx0hdxnz0002n0fy4ioqzyrp"},{"name":"ml","_id":"cjx0hfedz000088fyec7kilc8"}],"Data":[],"Page":[{"title":"tags","date":"2019-01-31T16:57:47.000Z","tags":["xiguashu","ml","testing"],"_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2019-02-01 00:57:47\ntags:\n  - xiguashu\n  - ml\n  - testing\n---\n","updated":"2019-02-20T02:23:39.441Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjv2f7ap40001jwfyzi394xkl","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"categories","date":"2019-01-31T16:58:44.000Z","_content":"\n[西瓜书](../xiguashu/)","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2019-02-01 00:58:44\n---\n\n[西瓜书](../xiguashu/)","updated":"2019-02-20T02:23:39.440Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cjv2f7ap60002jwfykppt8lng","content":"<p><a href=\"../xiguashu/\">西瓜书</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"../xiguashu/\">西瓜书</a></p>\n"}],"Post":[{"_content":"","source":"_posts/Untitled.md","raw":"","slug":"Untitled","published":1,"date":"2019-03-23T10:05:09.522Z","updated":"2019-04-29T13:42:23.724Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"cjv2f7al50000jwfydv4db77b","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Statistic","date":"2019-01-06T15:18:46.000Z","mathjax":true,"_content":"\n### Beyes\n\n$$P(H|D) = \\frac{P(H) \\cdot P(D|H)}{P(D)}​$$\n\nH: hyposis 假设事件，D: data 数据\n\n- $P(H|D)$ : 后验概率, the probability of observing event H given that D is true\n- $P(H)​$ : 先验概率, the probability of observing event H\n- $P(D|H)$: 似然度　\n- $P(D)$ :   the probability of data D\n\n例：在判断垃圾邮件的算法中:\n  $P(H)​$ : 所有邮件中，垃圾邮件的概率。\n  $P(D)​$ : 出现某个单词的概率。\n  $P(D|H)​$ : 垃圾邮件中，出现某个单词的概率。\n  $P(H|D)​$ : 出现某个单词的邮件，是垃圾邮件的概率。\n\n\n\n\n\n","source":"_posts/statitics/Statistics.md","raw":"---\ntitle: Statistic\ndate: 2019-01-06 23:18:46\ntags:\nmathjax: true\n---\n\n### Beyes\n\n$$P(H|D) = \\frac{P(H) \\cdot P(D|H)}{P(D)}​$$\n\nH: hyposis 假设事件，D: data 数据\n\n- $P(H|D)$ : 后验概率, the probability of observing event H given that D is true\n- $P(H)​$ : 先验概率, the probability of observing event H\n- $P(D|H)$: 似然度　\n- $P(D)$ :   the probability of data D\n\n例：在判断垃圾邮件的算法中:\n  $P(H)​$ : 所有邮件中，垃圾邮件的概率。\n  $P(D)​$ : 出现某个单词的概率。\n  $P(D|H)​$ : 垃圾邮件中，出现某个单词的概率。\n  $P(H|D)​$ : 出现某个单词的邮件，是垃圾邮件的概率。\n\n\n\n\n\n","slug":"statitics/Statistics","published":1,"updated":"2019-04-29T13:42:23.729Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjv2f7as80003jwfyr9r1bu5r","content":"<h3 id=\"Beyes\"><a href=\"#Beyes\" class=\"headerlink\" title=\"Beyes\"></a>Beyes</h3><p>$$P(H|D) = \\frac{P(H) \\cdot P(D|H)}{P(D)}​$$</p>\n<p>H: hyposis 假设事件，D: data 数据</p>\n<ul>\n<li>$P(H|D)$ : 后验概率, the probability of observing event H given that D is true</li>\n<li>$P(H)​$ : 先验概率, the probability of observing event H</li>\n<li>$P(D|H)$: 似然度　</li>\n<li>$P(D)$ :   the probability of data D</li>\n</ul>\n<p>例：在判断垃圾邮件的算法中:<br>  $P(H)​$ : 所有邮件中，垃圾邮件的概率。<br>  $P(D)​$ : 出现某个单词的概率。<br>  $P(D|H)​$ : 垃圾邮件中，出现某个单词的概率。<br>  $P(H|D)​$ : 出现某个单词的邮件，是垃圾邮件的概率。</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Beyes\"><a href=\"#Beyes\" class=\"headerlink\" title=\"Beyes\"></a>Beyes</h3><p>$$P(H|D) = \\frac{P(H) \\cdot P(D|H)}{P(D)}​$$</p>\n<p>H: hyposis 假设事件，D: data 数据</p>\n<ul>\n<li>$P(H|D)$ : 后验概率, the probability of observing event H given that D is true</li>\n<li>$P(H)​$ : 先验概率, the probability of observing event H</li>\n<li>$P(D|H)$: 似然度　</li>\n<li>$P(D)$ :   the probability of data D</li>\n</ul>\n<p>例：在判断垃圾邮件的算法中:<br>  $P(H)​$ : 所有邮件中，垃圾邮件的概率。<br>  $P(D)​$ : 出现某个单词的概率。<br>  $P(D|H)​$ : 垃圾邮件中，出现某个单词的概率。<br>  $P(H|D)​$ : 出现某个单词的邮件，是垃圾邮件的概率。</p>\n"},{"title":"无偏估计和有偏估计","date":"2019-04-29T13:42:23.730Z","_content":"\n","source":"_posts/statitics/estimate.md","raw":"---\ntitle: 无偏估计和有偏估计\ndate: \ncategories:\n  - \ntag: \n  - statistics\n---\n\n","slug":"statitics/estimate","published":1,"updated":"2019-04-29T13:45:57.230Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjv2f7asa0005jwfyi0bkc62o","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"entropy","date":"2019-03-25T15:18:46.000Z","_content":"\n### 信息量\n\n信息量是对事件发生概率的度量\n\n$log\\frac{1}{p}​$\n\n一个事件发生的概率越低，则这个事件包含的信息量越大，比如说越稀奇新闻包含的信息量越大，因为这种新闻出现的概率低。\n\n引用：一本五十万字的中文书平均有多少信息量？（$\\log_2m$）  《数学之美 -  吴军》\n我们知道常用的汉字（一级二级国标）大约有 7000 字。假如每个字等概率，那么我们大约需要 13 个比特（即 13 位二进制数）表示一个汉字［这里把汉字作为一个随机变量，那么汉字系统的熵就是约13bit］ 。但汉字的使用是不平衡的。实际上，前 10% 的汉字占文本的 95% 以上。因此，即使不考虑上下文的相关性，而只考虑每个汉字的独立的概率，那么，每个汉字的信息熵大约也只有 8-9 个比特。如果我们再考虑上下文相关性，每个汉字的信息熵［其实指的是汉字变量取特定汉字作为值时候具有的信息量］ 只有5比特左右。所以，一本五十万字的中文书，信息量大约是 250 万比特［这个时候的信息量就是每个汉字的信息量和数目相乘，指的都是汉字变量取具体值的信息量］ 。如果用一个好的算法压缩一下，整本书可以存成一个 320KB 的文件。如果我们直接用两字节的国标编码存储这本书，大约需要 1MB 大小，是压缩文件的三倍。这两个数量的差距，在信息论中称作“冗余度”（redundancy)。 需要指出的是我们这里讲的 250 万比特是个平均数，同样长度的书，所含的信息量可以差很多。如果一本书重复的内容很多，它的信息量就小，冗余度就大。\n\n### 熵（Entropy）\n\n信息论中，熵是接收每条消息中包含的信息量($\\log\\frac{1}{p}​$)的平均值。\n\n熵定义为信息的期望值（香农 shannon）：\n\n$$\\begin{align} H(X) & = E[I(X)] \\\\ & = E[-ln(P(X))]  \\\\ & = -\\sum\\limits_{i}^n P(x_i)logP(x_i)\\end{align} ​$$\n\n表示样本的不确定性量度。在信息世界，熵越高，则能传输越多的信息，熵越低，则意味着传输的信息越少。\n\n单位：\n\n熵的单位通常为比特, bit 或者sh(annon) (基于2)，但也用nat(基于自然对数)、Hart（基于10）计量，取决于定义用到对数的底。\n\n- 离散均匀分布\n\n$H(X)=log_2{m}$\n\n例：抛掷三枚硬币\n\n信息量：可以得到$2^3=8$种情况\n\n不确定性：$log_2{8}=2\\ bit$\n\n- 离散分布\n\n$H(X)=\\sum\\limits_{i}^nP(x_i)log\\frac{1}{P(x_i)}$\n\n例：选项ABCD\n\n信息量：$H(X)=\\sum\\limits_{i}^n\\frac{1}{4}log_2(\\frac{1}{4})=2 bit$\n\n给定A是错的，信息量变为：$H(X)=\\sum\\limits_{i}^n\\frac{1}{3}log_2(\\frac{1}{3})=1.585 bit$\n\n“A是错的”，提供了$2-1.585=0.415bit​$的信息\n\n例：编码\n\n分布p=(1/2, 1/2, 0, 0)，即A和B出现的概率均为1/2，C和D出现的概率都为0。计算H(p)为1，即只需要1位编码即可识别A和B\n\n分布q=(1/4, 1/4, 1/4, 1/4)来编码则得到H(q)=2，即需要2位编码来识别A和B，还有C和D\n\n### [交叉熵](https://zh.wikipedia.org/wiki/%E4%BA%A4%E5%8F%89%E7%86%B5)\n\n使用分布 q 来预测真实分布 p 的平均编码长度\n\n$H(p,q)= E_p[\\log  \\frac{1}{q(i)}] = \\sum\\limits_{i}^{} p(i)*log\\frac{1}{q(i)} $\n\n交叉熵可以看作每个信息片段在错误分布 q 下的期望编码位长度，而信息实际分布为 p。\n\n现有关于样本集的2个概率分布p和q，其中p为真实分布，q预测分布。\n\n按照真实分布p来衡量识别一个样本的所需要的编码长度的期望(即平均编码长度)为：$H(p)=\\sum\\limits_{i}^{} p(i)*log\\frac{1}{p(i)}$\n\n使用预测分布q来表示来自真实分布p的平均编码长度，则应该是：$H(p,q)=\\sum\\limits_{i}^{} p(i)*log\\frac{1}{q(i)} $\n\n因为用q来编码的样本来自实际分布p，所以期望H(p,q)中概率是p(i)。H(p,q)我们称之为“交叉熵”。\n\n比如含有4个字母(A,B,C,D)的数据集中，真实分布p=(1/2, 1/2, 0, 0)，即A和B出现的概率均为1/2，C和D出现的概率都为0，计算H(p)为1，即只需要1位编码即可识别A和B。如果使用分布Q=(1/4, 1/4, 1/4, 1/4)来编码则得到H(p,q)=2，即需要2位编码来识别A和B(C和D并不会出现)。\n\n根据非真实分布q得到的平均编码长度H(p,q)大于根据真实分布p得到的平均编码长度H(p)。事实上，根据[Gibbs' inequality](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Gibbs%2527_inequality)可知，H(p,q)>=H(p)恒成立，当q为真实分布p时取等号。\n\n### KL散度（相对熵）\n\n衡量分布 p 和 q 的差异，使用分布 q 来近似分布 p\n\n$D_{KL}(p||q)=H(p,q)-H(p) = \\sum\\limits_{i}^{} p(i)*\\log\\frac{1}{q(i)} - \\sum\\limits_{i}^{} p(i) *\\log\\frac{1}{p(i)} = \\sum\\limits_{i}^{} p(i)*\\log\\frac{p(i)}{q(i)}$\n\n我们将由q得到的平均编码长度比由p得到的平均编码长度多出的bit数称为“相对熵”，又被称为KL散度(Kullback–Leibler divergence，KLD) [Kullback–Leibler divergence](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Kullback%25E2%2580%2593Leibler_divergence)。它表示2个函数或概率分布的差异性：差异越大则相对熵越大，差异越小则相对熵越小，二者分布相同则相对熵为0。\n\n可以得到，交叉熵 $H(p,q)= E_p[\\log  \\frac{1}{q(i)}] = H(p) + D_{KL}(p||q)$\n\n其中 $H(p)$ 是 $p$ 的[熵](https://zh.wikipedia.org/wiki/%E4%BF%A1%E6%81%AF%E7%86%B5)，$D_{KL}(p\\|q)$ 是从 $p$ 到 $q$ 的[KL散度](https://zh.wikipedia.org/w/index.php?title=KL%E6%95%A3%E5%BA%A6&action=edit&redlink=1)(也被称为*p*相对于*q*的*相对熵*)。\n\n通常“相对熵”也可称为“交叉熵”，因为真实分布p是固定的，D(p||q)由H(p,q)决定。当然也有特殊情况，彼时2者须区别对待。\n\n### 交叉熵作为损失函数\n\n交叉熵可在神经网络(机器学习)中作为损失函数，p表示真实标记的分布，q则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量p与q的相似性。\n\n交叉熵和均方误差都可以作为神经网络的损失函数，他们的区别在于：\n\n1. 交叉熵适用于分类问题，结果是离散的类别（如图片分类），而均方误差适用于回归问题，结果是一个连续的数值（如雨量预测）【实际上均方误差也可以用于分类问题】\n2. 在使用 sigmod 激活函数时，如果使用均方误差作为损失函数，反向传播的导数（直接影响学习速度）会包含 sigmod函数的梯度，这个梯度随着变量的增大会趋向于0，导致学习速度迅速降低(梯度消失)；而如果使用交叉熵作为损失函数，就不存在这个问题，反向传播的导数包含 sigmod 函数，而不包含 sigmod 函数的导数。\n\n```python\n# softmax_cross_entropy_with_logits计算后，矩阵的每一行数据计算出一个交叉熵，三行数据共计算出三个交叉熵\ncross_entropy_lst = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_)\n# 通过reduce_sum进行累加，计算出一个batch的交叉熵\ncross_entropy = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))\n# 将batch里每条记录的交叉熵求均值，作为损失\ncross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))\n\n>>>\n[ 0.40760595  0.40760595  0.40760595]\n1.22282\n0.407606\n```\n\n\n\n\n\nReferences:\n- https://www.zhihu.com/question/41252833\n- https://zh.wikipedia.org/wiki/%E4%BA%A4%E5%8F%89%E7%86%B5\n- https://www.jianshu.com/p/92220ab37ea3","source":"_posts/statitics/entropy.md","raw":"---\ntitle: entropy\ndate: 2019-03-25 23:18:46\ncategories:\n  - math\ntag: \n  - math\n---\n\n### 信息量\n\n信息量是对事件发生概率的度量\n\n$log\\frac{1}{p}​$\n\n一个事件发生的概率越低，则这个事件包含的信息量越大，比如说越稀奇新闻包含的信息量越大，因为这种新闻出现的概率低。\n\n引用：一本五十万字的中文书平均有多少信息量？（$\\log_2m$）  《数学之美 -  吴军》\n我们知道常用的汉字（一级二级国标）大约有 7000 字。假如每个字等概率，那么我们大约需要 13 个比特（即 13 位二进制数）表示一个汉字［这里把汉字作为一个随机变量，那么汉字系统的熵就是约13bit］ 。但汉字的使用是不平衡的。实际上，前 10% 的汉字占文本的 95% 以上。因此，即使不考虑上下文的相关性，而只考虑每个汉字的独立的概率，那么，每个汉字的信息熵大约也只有 8-9 个比特。如果我们再考虑上下文相关性，每个汉字的信息熵［其实指的是汉字变量取特定汉字作为值时候具有的信息量］ 只有5比特左右。所以，一本五十万字的中文书，信息量大约是 250 万比特［这个时候的信息量就是每个汉字的信息量和数目相乘，指的都是汉字变量取具体值的信息量］ 。如果用一个好的算法压缩一下，整本书可以存成一个 320KB 的文件。如果我们直接用两字节的国标编码存储这本书，大约需要 1MB 大小，是压缩文件的三倍。这两个数量的差距，在信息论中称作“冗余度”（redundancy)。 需要指出的是我们这里讲的 250 万比特是个平均数，同样长度的书，所含的信息量可以差很多。如果一本书重复的内容很多，它的信息量就小，冗余度就大。\n\n### 熵（Entropy）\n\n信息论中，熵是接收每条消息中包含的信息量($\\log\\frac{1}{p}​$)的平均值。\n\n熵定义为信息的期望值（香农 shannon）：\n\n$$\\begin{align} H(X) & = E[I(X)] \\\\ & = E[-ln(P(X))]  \\\\ & = -\\sum\\limits_{i}^n P(x_i)logP(x_i)\\end{align} ​$$\n\n表示样本的不确定性量度。在信息世界，熵越高，则能传输越多的信息，熵越低，则意味着传输的信息越少。\n\n单位：\n\n熵的单位通常为比特, bit 或者sh(annon) (基于2)，但也用nat(基于自然对数)、Hart（基于10）计量，取决于定义用到对数的底。\n\n- 离散均匀分布\n\n$H(X)=log_2{m}$\n\n例：抛掷三枚硬币\n\n信息量：可以得到$2^3=8$种情况\n\n不确定性：$log_2{8}=2\\ bit$\n\n- 离散分布\n\n$H(X)=\\sum\\limits_{i}^nP(x_i)log\\frac{1}{P(x_i)}$\n\n例：选项ABCD\n\n信息量：$H(X)=\\sum\\limits_{i}^n\\frac{1}{4}log_2(\\frac{1}{4})=2 bit$\n\n给定A是错的，信息量变为：$H(X)=\\sum\\limits_{i}^n\\frac{1}{3}log_2(\\frac{1}{3})=1.585 bit$\n\n“A是错的”，提供了$2-1.585=0.415bit​$的信息\n\n例：编码\n\n分布p=(1/2, 1/2, 0, 0)，即A和B出现的概率均为1/2，C和D出现的概率都为0。计算H(p)为1，即只需要1位编码即可识别A和B\n\n分布q=(1/4, 1/4, 1/4, 1/4)来编码则得到H(q)=2，即需要2位编码来识别A和B，还有C和D\n\n### [交叉熵](https://zh.wikipedia.org/wiki/%E4%BA%A4%E5%8F%89%E7%86%B5)\n\n使用分布 q 来预测真实分布 p 的平均编码长度\n\n$H(p,q)= E_p[\\log  \\frac{1}{q(i)}] = \\sum\\limits_{i}^{} p(i)*log\\frac{1}{q(i)} $\n\n交叉熵可以看作每个信息片段在错误分布 q 下的期望编码位长度，而信息实际分布为 p。\n\n现有关于样本集的2个概率分布p和q，其中p为真实分布，q预测分布。\n\n按照真实分布p来衡量识别一个样本的所需要的编码长度的期望(即平均编码长度)为：$H(p)=\\sum\\limits_{i}^{} p(i)*log\\frac{1}{p(i)}$\n\n使用预测分布q来表示来自真实分布p的平均编码长度，则应该是：$H(p,q)=\\sum\\limits_{i}^{} p(i)*log\\frac{1}{q(i)} $\n\n因为用q来编码的样本来自实际分布p，所以期望H(p,q)中概率是p(i)。H(p,q)我们称之为“交叉熵”。\n\n比如含有4个字母(A,B,C,D)的数据集中，真实分布p=(1/2, 1/2, 0, 0)，即A和B出现的概率均为1/2，C和D出现的概率都为0，计算H(p)为1，即只需要1位编码即可识别A和B。如果使用分布Q=(1/4, 1/4, 1/4, 1/4)来编码则得到H(p,q)=2，即需要2位编码来识别A和B(C和D并不会出现)。\n\n根据非真实分布q得到的平均编码长度H(p,q)大于根据真实分布p得到的平均编码长度H(p)。事实上，根据[Gibbs' inequality](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Gibbs%2527_inequality)可知，H(p,q)>=H(p)恒成立，当q为真实分布p时取等号。\n\n### KL散度（相对熵）\n\n衡量分布 p 和 q 的差异，使用分布 q 来近似分布 p\n\n$D_{KL}(p||q)=H(p,q)-H(p) = \\sum\\limits_{i}^{} p(i)*\\log\\frac{1}{q(i)} - \\sum\\limits_{i}^{} p(i) *\\log\\frac{1}{p(i)} = \\sum\\limits_{i}^{} p(i)*\\log\\frac{p(i)}{q(i)}$\n\n我们将由q得到的平均编码长度比由p得到的平均编码长度多出的bit数称为“相对熵”，又被称为KL散度(Kullback–Leibler divergence，KLD) [Kullback–Leibler divergence](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Kullback%25E2%2580%2593Leibler_divergence)。它表示2个函数或概率分布的差异性：差异越大则相对熵越大，差异越小则相对熵越小，二者分布相同则相对熵为0。\n\n可以得到，交叉熵 $H(p,q)= E_p[\\log  \\frac{1}{q(i)}] = H(p) + D_{KL}(p||q)$\n\n其中 $H(p)$ 是 $p$ 的[熵](https://zh.wikipedia.org/wiki/%E4%BF%A1%E6%81%AF%E7%86%B5)，$D_{KL}(p\\|q)$ 是从 $p$ 到 $q$ 的[KL散度](https://zh.wikipedia.org/w/index.php?title=KL%E6%95%A3%E5%BA%A6&action=edit&redlink=1)(也被称为*p*相对于*q*的*相对熵*)。\n\n通常“相对熵”也可称为“交叉熵”，因为真实分布p是固定的，D(p||q)由H(p,q)决定。当然也有特殊情况，彼时2者须区别对待。\n\n### 交叉熵作为损失函数\n\n交叉熵可在神经网络(机器学习)中作为损失函数，p表示真实标记的分布，q则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量p与q的相似性。\n\n交叉熵和均方误差都可以作为神经网络的损失函数，他们的区别在于：\n\n1. 交叉熵适用于分类问题，结果是离散的类别（如图片分类），而均方误差适用于回归问题，结果是一个连续的数值（如雨量预测）【实际上均方误差也可以用于分类问题】\n2. 在使用 sigmod 激活函数时，如果使用均方误差作为损失函数，反向传播的导数（直接影响学习速度）会包含 sigmod函数的梯度，这个梯度随着变量的增大会趋向于0，导致学习速度迅速降低(梯度消失)；而如果使用交叉熵作为损失函数，就不存在这个问题，反向传播的导数包含 sigmod 函数，而不包含 sigmod 函数的导数。\n\n```python\n# softmax_cross_entropy_with_logits计算后，矩阵的每一行数据计算出一个交叉熵，三行数据共计算出三个交叉熵\ncross_entropy_lst = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_)\n# 通过reduce_sum进行累加，计算出一个batch的交叉熵\ncross_entropy = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))\n# 将batch里每条记录的交叉熵求均值，作为损失\ncross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))\n\n>>>\n[ 0.40760595  0.40760595  0.40760595]\n1.22282\n0.407606\n```\n\n\n\n\n\nReferences:\n- https://www.zhihu.com/question/41252833\n- https://zh.wikipedia.org/wiki/%E4%BA%A4%E5%8F%89%E7%86%B5\n- https://www.jianshu.com/p/92220ab37ea3","slug":"statitics/entropy","published":1,"updated":"2019-06-17T11:26:21.252Z","_id":"cjv2f7asf0009jwfynndvnlbx","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"信息量\"><a href=\"#信息量\" class=\"headerlink\" title=\"信息量\"></a>信息量</h3><p>信息量是对事件发生概率的度量</p>\n<p>$log\\frac{1}{p}​$</p>\n<p>一个事件发生的概率越低，则这个事件包含的信息量越大，比如说越稀奇新闻包含的信息量越大，因为这种新闻出现的概率低。</p>\n<p>引用：一本五十万字的中文书平均有多少信息量？（$\\log_2m$）  《数学之美 -  吴军》<br>我们知道常用的汉字（一级二级国标）大约有 7000 字。假如每个字等概率，那么我们大约需要 13 个比特（即 13 位二进制数）表示一个汉字［这里把汉字作为一个随机变量，那么汉字系统的熵就是约13bit］ 。但汉字的使用是不平衡的。实际上，前 10% 的汉字占文本的 95% 以上。因此，即使不考虑上下文的相关性，而只考虑每个汉字的独立的概率，那么，每个汉字的信息熵大约也只有 8-9 个比特。如果我们再考虑上下文相关性，每个汉字的信息熵［其实指的是汉字变量取特定汉字作为值时候具有的信息量］ 只有5比特左右。所以，一本五十万字的中文书，信息量大约是 250 万比特［这个时候的信息量就是每个汉字的信息量和数目相乘，指的都是汉字变量取具体值的信息量］ 。如果用一个好的算法压缩一下，整本书可以存成一个 320KB 的文件。如果我们直接用两字节的国标编码存储这本书，大约需要 1MB 大小，是压缩文件的三倍。这两个数量的差距，在信息论中称作“冗余度”（redundancy)。 需要指出的是我们这里讲的 250 万比特是个平均数，同样长度的书，所含的信息量可以差很多。如果一本书重复的内容很多，它的信息量就小，冗余度就大。</p>\n<h3 id=\"熵（Entropy）\"><a href=\"#熵（Entropy）\" class=\"headerlink\" title=\"熵（Entropy）\"></a>熵（Entropy）</h3><p>信息论中，熵是接收每条消息中包含的信息量($\\log\\frac{1}{p}​$)的平均值。</p>\n<p>熵定义为信息的期望值（香农 shannon）：</p>\n<p>$$\\begin{align} H(X) &amp; = E[I(X)] \\ &amp; = E[-ln(P(X))]  \\ &amp; = -\\sum\\limits_{i}^n P(x_i)logP(x_i)\\end{align} ​$$</p>\n<p>表示样本的不确定性量度。在信息世界，熵越高，则能传输越多的信息，熵越低，则意味着传输的信息越少。</p>\n<p>单位：</p>\n<p>熵的单位通常为比特, bit 或者sh(annon) (基于2)，但也用nat(基于自然对数)、Hart（基于10）计量，取决于定义用到对数的底。</p>\n<ul>\n<li>离散均匀分布</li>\n</ul>\n<p>$H(X)=log_2{m}$</p>\n<p>例：抛掷三枚硬币</p>\n<p>信息量：可以得到$2^3=8$种情况</p>\n<p>不确定性：$log_2{8}=2\\ bit$</p>\n<ul>\n<li>离散分布</li>\n</ul>\n<p>$H(X)=\\sum\\limits_{i}^nP(x_i)log\\frac{1}{P(x_i)}$</p>\n<p>例：选项ABCD</p>\n<p>信息量：$H(X)=\\sum\\limits_{i}^n\\frac{1}{4}log_2(\\frac{1}{4})=2 bit$</p>\n<p>给定A是错的，信息量变为：$H(X)=\\sum\\limits_{i}^n\\frac{1}{3}log_2(\\frac{1}{3})=1.585 bit$</p>\n<p>“A是错的”，提供了$2-1.585=0.415bit​$的信息</p>\n<p>例：编码</p>\n<p>分布p=(1/2, 1/2, 0, 0)，即A和B出现的概率均为1/2，C和D出现的概率都为0。计算H(p)为1，即只需要1位编码即可识别A和B</p>\n<p>分布q=(1/4, 1/4, 1/4, 1/4)来编码则得到H(q)=2，即需要2位编码来识别A和B，还有C和D</p>\n<h3 id=\"交叉熵\"><a href=\"#交叉熵\" class=\"headerlink\" title=\"交叉熵\"></a><a href=\"https://zh.wikipedia.org/wiki/%E4%BA%A4%E5%8F%89%E7%86%B5\" target=\"_blank\" rel=\"noopener\">交叉熵</a></h3><p>使用分布 q 来预测真实分布 p 的平均编码长度</p>\n<p>$H(p,q)= E_p[\\log  \\frac{1}{q(i)}] = \\sum\\limits_{i}^{} p(i)*log\\frac{1}{q(i)} $</p>\n<p>交叉熵可以看作每个信息片段在错误分布 q 下的期望编码位长度，而信息实际分布为 p。</p>\n<p>现有关于样本集的2个概率分布p和q，其中p为真实分布，q预测分布。</p>\n<p>按照真实分布p来衡量识别一个样本的所需要的编码长度的期望(即平均编码长度)为：$H(p)=\\sum\\limits_{i}^{} p(i)*log\\frac{1}{p(i)}$</p>\n<p>使用预测分布q来表示来自真实分布p的平均编码长度，则应该是：$H(p,q)=\\sum\\limits_{i}^{} p(i)*log\\frac{1}{q(i)} $</p>\n<p>因为用q来编码的样本来自实际分布p，所以期望H(p,q)中概率是p(i)。H(p,q)我们称之为“交叉熵”。</p>\n<p>比如含有4个字母(A,B,C,D)的数据集中，真实分布p=(1/2, 1/2, 0, 0)，即A和B出现的概率均为1/2，C和D出现的概率都为0，计算H(p)为1，即只需要1位编码即可识别A和B。如果使用分布Q=(1/4, 1/4, 1/4, 1/4)来编码则得到H(p,q)=2，即需要2位编码来识别A和B(C和D并不会出现)。</p>\n<p>根据非真实分布q得到的平均编码长度H(p,q)大于根据真实分布p得到的平均编码长度H(p)。事实上，根据<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Gibbs%2527_inequality\" target=\"_blank\" rel=\"noopener\">Gibbs’ inequality</a>可知，H(p,q)&gt;=H(p)恒成立，当q为真实分布p时取等号。</p>\n<h3 id=\"KL散度（相对熵）\"><a href=\"#KL散度（相对熵）\" class=\"headerlink\" title=\"KL散度（相对熵）\"></a>KL散度（相对熵）</h3><p>衡量分布 p 和 q 的差异，使用分布 q 来近似分布 p</p>\n<p>$D_{KL}(p||q)=H(p,q)-H(p) = \\sum\\limits_{i}^{} p(i)<em>\\log\\frac{1}{q(i)} - \\sum\\limits_{i}^{} p(i) </em>\\log\\frac{1}{p(i)} = \\sum\\limits_{i}^{} p(i)*\\log\\frac{p(i)}{q(i)}$</p>\n<p>我们将由q得到的平均编码长度比由p得到的平均编码长度多出的bit数称为“相对熵”，又被称为KL散度(Kullback–Leibler divergence，KLD) <a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Kullback%25E2%2580%2593Leibler_divergence\" target=\"_blank\" rel=\"noopener\">Kullback–Leibler divergence</a>。它表示2个函数或概率分布的差异性：差异越大则相对熵越大，差异越小则相对熵越小，二者分布相同则相对熵为0。</p>\n<p>可以得到，交叉熵 $H(p,q)= E_p[\\log  \\frac{1}{q(i)}] = H(p) + D_{KL}(p||q)$</p>\n<p>其中 $H(p)$ 是 $p$ 的<a href=\"https://zh.wikipedia.org/wiki/%E4%BF%A1%E6%81%AF%E7%86%B5\" target=\"_blank\" rel=\"noopener\">熵</a>，$D_{KL}(p|q)$ 是从 $p$ 到 $q$ 的<a href=\"https://zh.wikipedia.org/w/index.php?title=KL%E6%95%A3%E5%BA%A6&amp;action=edit&amp;redlink=1\" target=\"_blank\" rel=\"noopener\">KL散度</a>(也被称为<em>p</em>相对于<em>q</em>的<em>相对熵</em>)。</p>\n<p>通常“相对熵”也可称为“交叉熵”，因为真实分布p是固定的，D(p||q)由H(p,q)决定。当然也有特殊情况，彼时2者须区别对待。</p>\n<h3 id=\"交叉熵作为损失函数\"><a href=\"#交叉熵作为损失函数\" class=\"headerlink\" title=\"交叉熵作为损失函数\"></a>交叉熵作为损失函数</h3><p>交叉熵可在神经网络(机器学习)中作为损失函数，p表示真实标记的分布，q则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量p与q的相似性。</p>\n<p>交叉熵和均方误差都可以作为神经网络的损失函数，他们的区别在于：</p>\n<ol>\n<li>交叉熵适用于分类问题，结果是离散的类别（如图片分类），而均方误差适用于回归问题，结果是一个连续的数值（如雨量预测）【实际上均方误差也可以用于分类问题】</li>\n<li>在使用 sigmod 激活函数时，如果使用均方误差作为损失函数，反向传播的导数（直接影响学习速度）会包含 sigmod函数的梯度，这个梯度随着变量的增大会趋向于0，导致学习速度迅速降低(梯度消失)；而如果使用交叉熵作为损失函数，就不存在这个问题，反向传播的导数包含 sigmod 函数，而不包含 sigmod 函数的导数。</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># softmax_cross_entropy_with_logits计算后，矩阵的每一行数据计算出一个交叉熵，三行数据共计算出三个交叉熵</span></span><br><span class=\"line\">cross_entropy_lst = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_)</span><br><span class=\"line\"><span class=\"comment\"># 通过reduce_sum进行累加，计算出一个batch的交叉熵</span></span><br><span class=\"line\">cross_entropy = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))</span><br><span class=\"line\"><span class=\"comment\"># 将batch里每条记录的交叉熵求均值，作为损失</span></span><br><span class=\"line\">cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt;</span><br><span class=\"line\">[ <span class=\"number\">0.40760595</span>  <span class=\"number\">0.40760595</span>  <span class=\"number\">0.40760595</span>]</span><br><span class=\"line\"><span class=\"number\">1.22282</span></span><br><span class=\"line\"><span class=\"number\">0.407606</span></span><br></pre></td></tr></table></figure>\n<p>References:</p>\n<ul>\n<li><a href=\"https://www.zhihu.com/question/41252833\" target=\"_blank\" rel=\"noopener\">https://www.zhihu.com/question/41252833</a></li>\n<li><a href=\"https://zh.wikipedia.org/wiki/%E4%BA%A4%E5%8F%89%E7%86%B5\" target=\"_blank\" rel=\"noopener\">https://zh.wikipedia.org/wiki/%E4%BA%A4%E5%8F%89%E7%86%B5</a></li>\n<li><a href=\"https://www.jianshu.com/p/92220ab37ea3\" target=\"_blank\" rel=\"noopener\">https://www.jianshu.com/p/92220ab37ea3</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"信息量\"><a href=\"#信息量\" class=\"headerlink\" title=\"信息量\"></a>信息量</h3><p>信息量是对事件发生概率的度量</p>\n<p>$log\\frac{1}{p}​$</p>\n<p>一个事件发生的概率越低，则这个事件包含的信息量越大，比如说越稀奇新闻包含的信息量越大，因为这种新闻出现的概率低。</p>\n<p>引用：一本五十万字的中文书平均有多少信息量？（$\\log_2m$）  《数学之美 -  吴军》<br>我们知道常用的汉字（一级二级国标）大约有 7000 字。假如每个字等概率，那么我们大约需要 13 个比特（即 13 位二进制数）表示一个汉字［这里把汉字作为一个随机变量，那么汉字系统的熵就是约13bit］ 。但汉字的使用是不平衡的。实际上，前 10% 的汉字占文本的 95% 以上。因此，即使不考虑上下文的相关性，而只考虑每个汉字的独立的概率，那么，每个汉字的信息熵大约也只有 8-9 个比特。如果我们再考虑上下文相关性，每个汉字的信息熵［其实指的是汉字变量取特定汉字作为值时候具有的信息量］ 只有5比特左右。所以，一本五十万字的中文书，信息量大约是 250 万比特［这个时候的信息量就是每个汉字的信息量和数目相乘，指的都是汉字变量取具体值的信息量］ 。如果用一个好的算法压缩一下，整本书可以存成一个 320KB 的文件。如果我们直接用两字节的国标编码存储这本书，大约需要 1MB 大小，是压缩文件的三倍。这两个数量的差距，在信息论中称作“冗余度”（redundancy)。 需要指出的是我们这里讲的 250 万比特是个平均数，同样长度的书，所含的信息量可以差很多。如果一本书重复的内容很多，它的信息量就小，冗余度就大。</p>\n<h3 id=\"熵（Entropy）\"><a href=\"#熵（Entropy）\" class=\"headerlink\" title=\"熵（Entropy）\"></a>熵（Entropy）</h3><p>信息论中，熵是接收每条消息中包含的信息量($\\log\\frac{1}{p}​$)的平均值。</p>\n<p>熵定义为信息的期望值（香农 shannon）：</p>\n<p>$$\\begin{align} H(X) &amp; = E[I(X)] \\ &amp; = E[-ln(P(X))]  \\ &amp; = -\\sum\\limits_{i}^n P(x_i)logP(x_i)\\end{align} ​$$</p>\n<p>表示样本的不确定性量度。在信息世界，熵越高，则能传输越多的信息，熵越低，则意味着传输的信息越少。</p>\n<p>单位：</p>\n<p>熵的单位通常为比特, bit 或者sh(annon) (基于2)，但也用nat(基于自然对数)、Hart（基于10）计量，取决于定义用到对数的底。</p>\n<ul>\n<li>离散均匀分布</li>\n</ul>\n<p>$H(X)=log_2{m}$</p>\n<p>例：抛掷三枚硬币</p>\n<p>信息量：可以得到$2^3=8$种情况</p>\n<p>不确定性：$log_2{8}=2\\ bit$</p>\n<ul>\n<li>离散分布</li>\n</ul>\n<p>$H(X)=\\sum\\limits_{i}^nP(x_i)log\\frac{1}{P(x_i)}$</p>\n<p>例：选项ABCD</p>\n<p>信息量：$H(X)=\\sum\\limits_{i}^n\\frac{1}{4}log_2(\\frac{1}{4})=2 bit$</p>\n<p>给定A是错的，信息量变为：$H(X)=\\sum\\limits_{i}^n\\frac{1}{3}log_2(\\frac{1}{3})=1.585 bit$</p>\n<p>“A是错的”，提供了$2-1.585=0.415bit​$的信息</p>\n<p>例：编码</p>\n<p>分布p=(1/2, 1/2, 0, 0)，即A和B出现的概率均为1/2，C和D出现的概率都为0。计算H(p)为1，即只需要1位编码即可识别A和B</p>\n<p>分布q=(1/4, 1/4, 1/4, 1/4)来编码则得到H(q)=2，即需要2位编码来识别A和B，还有C和D</p>\n<h3 id=\"交叉熵\"><a href=\"#交叉熵\" class=\"headerlink\" title=\"交叉熵\"></a><a href=\"https://zh.wikipedia.org/wiki/%E4%BA%A4%E5%8F%89%E7%86%B5\" target=\"_blank\" rel=\"noopener\">交叉熵</a></h3><p>使用分布 q 来预测真实分布 p 的平均编码长度</p>\n<p>$H(p,q)= E_p[\\log  \\frac{1}{q(i)}] = \\sum\\limits_{i}^{} p(i)*log\\frac{1}{q(i)} $</p>\n<p>交叉熵可以看作每个信息片段在错误分布 q 下的期望编码位长度，而信息实际分布为 p。</p>\n<p>现有关于样本集的2个概率分布p和q，其中p为真实分布，q预测分布。</p>\n<p>按照真实分布p来衡量识别一个样本的所需要的编码长度的期望(即平均编码长度)为：$H(p)=\\sum\\limits_{i}^{} p(i)*log\\frac{1}{p(i)}$</p>\n<p>使用预测分布q来表示来自真实分布p的平均编码长度，则应该是：$H(p,q)=\\sum\\limits_{i}^{} p(i)*log\\frac{1}{q(i)} $</p>\n<p>因为用q来编码的样本来自实际分布p，所以期望H(p,q)中概率是p(i)。H(p,q)我们称之为“交叉熵”。</p>\n<p>比如含有4个字母(A,B,C,D)的数据集中，真实分布p=(1/2, 1/2, 0, 0)，即A和B出现的概率均为1/2，C和D出现的概率都为0，计算H(p)为1，即只需要1位编码即可识别A和B。如果使用分布Q=(1/4, 1/4, 1/4, 1/4)来编码则得到H(p,q)=2，即需要2位编码来识别A和B(C和D并不会出现)。</p>\n<p>根据非真实分布q得到的平均编码长度H(p,q)大于根据真实分布p得到的平均编码长度H(p)。事实上，根据<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Gibbs%2527_inequality\" target=\"_blank\" rel=\"noopener\">Gibbs’ inequality</a>可知，H(p,q)&gt;=H(p)恒成立，当q为真实分布p时取等号。</p>\n<h3 id=\"KL散度（相对熵）\"><a href=\"#KL散度（相对熵）\" class=\"headerlink\" title=\"KL散度（相对熵）\"></a>KL散度（相对熵）</h3><p>衡量分布 p 和 q 的差异，使用分布 q 来近似分布 p</p>\n<p>$D_{KL}(p||q)=H(p,q)-H(p) = \\sum\\limits_{i}^{} p(i)<em>\\log\\frac{1}{q(i)} - \\sum\\limits_{i}^{} p(i) </em>\\log\\frac{1}{p(i)} = \\sum\\limits_{i}^{} p(i)*\\log\\frac{p(i)}{q(i)}$</p>\n<p>我们将由q得到的平均编码长度比由p得到的平均编码长度多出的bit数称为“相对熵”，又被称为KL散度(Kullback–Leibler divergence，KLD) <a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Kullback%25E2%2580%2593Leibler_divergence\" target=\"_blank\" rel=\"noopener\">Kullback–Leibler divergence</a>。它表示2个函数或概率分布的差异性：差异越大则相对熵越大，差异越小则相对熵越小，二者分布相同则相对熵为0。</p>\n<p>可以得到，交叉熵 $H(p,q)= E_p[\\log  \\frac{1}{q(i)}] = H(p) + D_{KL}(p||q)$</p>\n<p>其中 $H(p)$ 是 $p$ 的<a href=\"https://zh.wikipedia.org/wiki/%E4%BF%A1%E6%81%AF%E7%86%B5\" target=\"_blank\" rel=\"noopener\">熵</a>，$D_{KL}(p|q)$ 是从 $p$ 到 $q$ 的<a href=\"https://zh.wikipedia.org/w/index.php?title=KL%E6%95%A3%E5%BA%A6&amp;action=edit&amp;redlink=1\" target=\"_blank\" rel=\"noopener\">KL散度</a>(也被称为<em>p</em>相对于<em>q</em>的<em>相对熵</em>)。</p>\n<p>通常“相对熵”也可称为“交叉熵”，因为真实分布p是固定的，D(p||q)由H(p,q)决定。当然也有特殊情况，彼时2者须区别对待。</p>\n<h3 id=\"交叉熵作为损失函数\"><a href=\"#交叉熵作为损失函数\" class=\"headerlink\" title=\"交叉熵作为损失函数\"></a>交叉熵作为损失函数</h3><p>交叉熵可在神经网络(机器学习)中作为损失函数，p表示真实标记的分布，q则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量p与q的相似性。</p>\n<p>交叉熵和均方误差都可以作为神经网络的损失函数，他们的区别在于：</p>\n<ol>\n<li>交叉熵适用于分类问题，结果是离散的类别（如图片分类），而均方误差适用于回归问题，结果是一个连续的数值（如雨量预测）【实际上均方误差也可以用于分类问题】</li>\n<li>在使用 sigmod 激活函数时，如果使用均方误差作为损失函数，反向传播的导数（直接影响学习速度）会包含 sigmod函数的梯度，这个梯度随着变量的增大会趋向于0，导致学习速度迅速降低(梯度消失)；而如果使用交叉熵作为损失函数，就不存在这个问题，反向传播的导数包含 sigmod 函数，而不包含 sigmod 函数的导数。</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># softmax_cross_entropy_with_logits计算后，矩阵的每一行数据计算出一个交叉熵，三行数据共计算出三个交叉熵</span></span><br><span class=\"line\">cross_entropy_lst = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_)</span><br><span class=\"line\"><span class=\"comment\"># 通过reduce_sum进行累加，计算出一个batch的交叉熵</span></span><br><span class=\"line\">cross_entropy = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))</span><br><span class=\"line\"><span class=\"comment\"># 将batch里每条记录的交叉熵求均值，作为损失</span></span><br><span class=\"line\">cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))</span><br><span class=\"line\"></span><br><span class=\"line\">&gt;&gt;&gt;</span><br><span class=\"line\">[ <span class=\"number\">0.40760595</span>  <span class=\"number\">0.40760595</span>  <span class=\"number\">0.40760595</span>]</span><br><span class=\"line\"><span class=\"number\">1.22282</span></span><br><span class=\"line\"><span class=\"number\">0.407606</span></span><br></pre></td></tr></table></figure>\n<p>References:</p>\n<ul>\n<li><a href=\"https://www.zhihu.com/question/41252833\" target=\"_blank\" rel=\"noopener\">https://www.zhihu.com/question/41252833</a></li>\n<li><a href=\"https://zh.wikipedia.org/wiki/%E4%BA%A4%E5%8F%89%E7%86%B5\" target=\"_blank\" rel=\"noopener\">https://zh.wikipedia.org/wiki/%E4%BA%A4%E5%8F%89%E7%86%B5</a></li>\n<li><a href=\"https://www.jianshu.com/p/92220ab37ea3\" target=\"_blank\" rel=\"noopener\">https://www.jianshu.com/p/92220ab37ea3</a></li>\n</ul>\n"},{"title":"Neuron Network","date":"2019-06-16T16:00:00.000Z","mathjax":true,"_content":"深度学习和神经网络(CS231n Note)\n---------------------------\n\n## 1. 神经元模型与数学模型（Neuron Network Unit）\n\n![img](nn_cs231n_note/d0cbce2f2654b8e70fe201fec2982c7d_hd.png)\n\n神经元（Neuron）通过树突（Dendrites）接收输入信号，沿着轴突（axon）产生输出信号。轴突在末端分叉，通过突触和其他神经元的树突相连。\n\n输入信号（$x_0,x_1,...,x_n$）传递到其他神经元的树突，基于突触的突触强度相乘（$w_0x_0,w_1x_1,...,w_nx_n$）。突触的强度（权重$w$）可以控制一个神经元对另一个神经元的影响强度，使其兴奋（正权重）或抑制（负权重）。输出信号如果高于阈值，则神经元激活（对应于激活函数$f(\\sum_{i}w_ix_i+b)$）。\n\n1. 1 多层感知机\n\n   输入层  -> 隐藏层 -> 输出层\n\n   ![img](nn_cs231n_note/ccb56c1fb267bc632d6d88459eb14ace_hd.png)\n\n## 2. 常用激活函数\n\n|         | 函数                                    | 值域          | 导数 | 备注 |\n| ------- | --------------------------------------- | ------------- | ---- | ---- |\n| sigmoid | $ \\sigma(x)=\\frac{1}{1+e^{-x}} $        | [0,1]         | [0, 0.25]   | 分类概率 |\n| tanh    | $tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$ | [-1,1]        | [0, 1]     |      |\n| relu    | $relu(x)=max(0, x)$                     | $[0,+\\infty]$ |$f'_x=\\begin{cases}0, x<0\\\\1, x>0\\\\undefinded, x=0\\end{cases}$||\n#### sigmoid\n\n优点：\n\n1. 分类概率\n\n缺点\n\n1. 梯度容易饱和而丢失，激活函数在接近0,1时会饱和，如果权重过大，很容易失去梯度\n2. 函数不经过0，不以0点对称，在中间点0.5附近的的梯度较小（0.25），梯度总体较小，不利于梯度传播\n3. 激活值永远全为正（负），下一神经元的输入总是正数（负数），则反向传播过程中梯度更新呈z字型\n4. exp指数函数计算复杂\n\n![1560776135809](nn_cs231n_note/1560776135809.png)\n\n#### tanh\n\n$tanh(x)=2\\sigma(2x)-1$\n\n优点：\n\n1. 范围更大 [0, 1]\n2. 以 0 为中心点\n3. 中间部分梯度更大，有利于梯度传播\n\n缺点：\n\n1. 梯度饱和而丢失的情况仍然存在\n\n   ### relu\n\n优点\n\n1. 收敛速度更快（e.g. 6x than sigmoid/tanh)\n2. 计算简单\n3. 梯度不会饱和\n\n缺点\n\n1. 可能导致部分神经元“死掉”，永远不会被激活。输出值始终为负，激活值为0，梯度为0，反向传播不更新此神经元的梯度。降低学习率来降低神经元“死掉”的概率。\n\n   \n\n![677187e96671a4cac9c95352743b3806_hd](nn_cs231n_note/677187e96671a4cac9c95352743b3806_hd.png)\n\n\n\n![677187e96671a4cac9c95352743b3806_hd](nn_cs231n_note/677187e96671a4cac9c95352743b3806_hd.png)![img](nn_cs231n_note/42.png)\n\n#### Leaky Relu\n\n解决Relu死亡的问题\n\n$$f(x)=\\begin{cases}x, x>0\\\\\\alpha x, x<0\\end{cases}$$, $\\alpha=0.01​$\n\n#### ELU\n\n$$f(x)=\\begin{cases} x, x>0\\\\ \\alpha(e^x-1), x\\leq0\\end{cases}$$\n\n![1560776449074](nn_cs231n_note/1560776449074.png)\n\n#### Maxout\n\n$$max(w_{1}^Tx+b_1, w_{2}^Tx+b_2)$$\n\n## 3. 数据预处理\n\n### 归一化 Normalization\n\n```\n# 数据归一化\nX = X / np.std(X, axis=0)\n# 维度归一化\nX = X / np.std(X, axis=1)\n```\n\n![img](nn_cs231n_note/e743b6777775b1671c3b5503d7afbbc4_hd.png)\n\n ### PCA 白化（很少在深度学习中使用）\n\nPCA/白化。**左边**是二维的原始数据。**中间**：经过PCA操作的数据。可以看出数据首先是零中心的，然后变换到了数据协方差矩阵的基准轴上。这样就对数据进行了解相关（协方差矩阵变成对角阵）。**右边**：每个维度都被特征值调整数值范围，将数据协方差矩阵变为单位矩阵。从几何上看，就是对数据在各个方向上拉伸压缩，使之变成服从高斯分布的一个数据点分布。\n\n![img](nn_cs231n_note/aae11de6e6a29f50d46b9ea106fbb02a_hd.png)\n\n### CIFAR 数据PCA\n\nnx3072 维向量（图片32x32x3）,协方差矩阵：3072x3072\n\n![img](nn_cs231n_note/8608c06086fc196228f4dda78499a2d9_hd.png)\n\n1: 49张图片。2: 3072个特征值向量中的前144个。3: 49张PCA降维的图片（U.transpose()[:144,:]）。4: 白化后的数据。144个维度的方差都压缩到相同的数值范围（U.transpose()[:144,:]）。现在较低的频率（代表了大多数方差）可以忽略不计了，较高的频率（代表相对少的方差）就被夸大了。\n\n## 4. 权重初始化\n\n#### 小随机数初始化\n\n基于均值为0，标准差为1的高斯分布\n\n```\nW = 0.01 * np.random.randn(N, D)\n```\n\n#### 使用$\\frac{1}{\\sqrt{n}} $校准方差\n\n数据量增大，随机初始化的神经元输出数据分布的方差也增大\n\n```python\nW = np.random.randn(N, D) / np.sqrt(N)\n```\n\n#### He Normal\n\n网络中神经元的方差应该是$\\frac{2}{n}$\n\n当前的推荐是使用ReLU激活函数，并且使用**w = np.random.randn(n) \\* sqrt(2.0/n)**来进行权重初始化\n\n```python\nW = np.random.randn(n) * sqrt(2.0/n)\n```\n\n#### 偏置初始化 biases\n\n```\nb = np.zeros(n,)\n```\n\n## 正则化\n\n#### L1 正则\n\n\n\n#### L2正则\n\n\n\n## Dropout\n\n![img](nn_cs231n_note/63fcf4cc655cb04f21a37e86aca333cf_hd.png)\n\n1.  Bagging 集成模型，随机抽样神经网络的子集。多个共享参数的子网络组成。\n2. 增强单个神经元独立学习特征的能力，减少神经元之间的依赖\n3. 加性噪声\n\n```\n\"\"\"\n反向随机失活: 推荐实现方式.\n在训练的时候drop和调整数值范围，测试时不做任何事.\n\"\"\"\n\np = 0.5 # 激活神经元的概率. p值更高 = 随机失活更弱\n\ndef train_step(X):\n  # 3层neural network的前向传播\n  H1 = np.maximum(0, np.dot(W1, X) + b1)\n  #                  [0, 1]随机分布 P(rand(x)) < p = p\n  mask1 = (np.random.rand(*H1.shape) < p) / p # 第一个随机失活掩码. 注意/p!\n  H1 *= mask1 # drop!\n  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n  mask2 = (np.random.rand(*H2.shape) < p) / p # 第二个随机失活掩码. 注意/p!\n  H2 *= mask2 # drop!\n  out = np.dot(W3, H2) + b3\n\n  # 反向传播:计算梯度... (略)\n  # 进行参数更新... (略)\n\ndef predict(X):\n  # 前向传播时模型集成\n  H1 = np.maximum(0, np.dot(W1, X) + b1) # 不用数值范围调整了\n  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n  out = np.dot(W3, H2) + b3\n```\n\n## Batch Normalization\n\n批量归一化可以理解为在网络的每一层之前都做预处理，减少之前网络权重对数据的影响，保持每一层输出数据的分布（均值和标准差），使输出适应下一层网络，也使得每一层数据相对独立。\n\n![1560779540116](nn_cs231n_note/1560779540116.png)\n\n### Internal Co-variate Shift\n\nReference: [Batch Normalization原理与实战](<https://zhuanlan.zhihu.com/p/34879333>)\n\n随着训练的进行，网络中的参数也随着梯度下降在不停更新。一方面，当底层网络中参数发生微弱变化时，由于每一层中的线性变换与非线性激活映射，这些微弱变化随着网络层数的加深而被放大（类似蝴蝶效应）；另一方面，参数的变化导致每一层的输入分布会发生改变，进而下一层的网络需要不停地去适应这些分布变化，使得我们的模型训练变得困难。上述这一现象叫做Internal Covariate Shift。\n\n原作定义：在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程被称作Internal Covariate Shift。\n\n随着梯度下降的进行，每一层的参数$W^{[l]}$与$b^{[l]}$都会被更新，那么$Z^{[l]}$的分布也就发生了改变，进而$A^{[l]}$也同样出现分布的改变。而$A^{[l]}$作为第 $l+1$ 层的输入，意味着 $l+1$ 层需要去不停适应这种数据分布的变化，这一过程叫做 Interval Covariate Shift.\n\n#### 带来的问题：\n\n1. 上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低\n2. 网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度（sigmoid, tanh）。 $Z^{[l]}$会逐渐更新并变大，陷入梯度饱和区。可以通过Normalization 使得激活函数输入分布在一个稳定的空间来避免他们陷入梯度饱和区。\n\n#### 如何减缓 Interval Covariate Shift\n\n1. 白化。成本高，改变了网络每一层分布导致数据表达的特征信息丢失\n\n   - 使得输入特征分布具有相同的均值与方差。其中PCA白化保证了所有特征分布均值为0，方差为1\n   - 去除特征之间的相关性\n\n2. Batch Normalization   简化加改进版的白化\n\n   - 简化。让每个特征都有均值为0，方差为1的分布就OK。\n   - 白化操作减弱了网络中每一层输入数据表达能力，那我就再加个线性变换操作，让这些数据再能够尽可能恢复本身的表达能力就好了。\n\n   ![1560779531173](nn_cs231n_note/1560779531173.png)\n\nBN 引入了两个可学习的参数 $\\gamma$ 和 $\\beta$.。这两个参数的引入是为了恢复数据本身的表达能力，对规范后的数据进行线性变换，即 $y_i = \\gamma x_i + \\beta_i$。 特别的，当 $\\gamma^2=\\sigma ^2$（方差）, $\\beta = \\mu$ （均值）时，可以实现等价变换并且保留原始输入特征的分布信息。\n\n#### Batch Normalization 的作用\n\n1. 使得网络中每层输入数据的分布相对稳定，加快模型学习速度\n\n2. 使得模型对参数不那么敏感，减小初始化参数对模型学习的影响，可以选择更大的初始化值，学习率选择范围更大\n\n   当学习率设置太高时，会使得参数更新步伐过大，容易出现震荡和不收敛。但是使用BN的网络将不会受到参数数值大小的影响。BN抑制了参数微小变化随着网络层数加深被放大的问题，使得网络对参数大小的适应能力更强\n\n3. 缓解梯度消失的问题\n\n4. 正则化效果，mini-batch 的mean/variance 作为总体样本的抽样估计，引入随机噪声\n\n**BN通过将每一层网络的输入进行normalization，保证输入分布的均值与方差固定在一定范围内，减少了网络中的Internal Covariate Shift问题，并在一定程度上缓解了梯度消失，加速了模型收敛；并且BN使得网络对参数、激活函数更加具有鲁棒性，降低了神经网络模型训练和调参的复杂度；最后BN训练过程中由于使用mini-batch的mean/variance作为总体样本统计量估计，引入了随机噪声，在一定程度上对模型起到了正则化的效果。**\n\n#### backpropagation\n\nReference: [Understanding the backward pass through Batch Normalization Layer](<https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html>)","source":"_posts/deep-learning/nn_cs231n_note.md","raw":"---\ntitle: Neuron Network\ndate: 2019-06-17\nmathjax: true\ncategories:\n  - dl\ntag:\n  - dl\n  - hexo-asset-image\n---\n深度学习和神经网络(CS231n Note)\n---------------------------\n\n## 1. 神经元模型与数学模型（Neuron Network Unit）\n\n![img](nn_cs231n_note/d0cbce2f2654b8e70fe201fec2982c7d_hd.png)\n\n神经元（Neuron）通过树突（Dendrites）接收输入信号，沿着轴突（axon）产生输出信号。轴突在末端分叉，通过突触和其他神经元的树突相连。\n\n输入信号（$x_0,x_1,...,x_n$）传递到其他神经元的树突，基于突触的突触强度相乘（$w_0x_0,w_1x_1,...,w_nx_n$）。突触的强度（权重$w$）可以控制一个神经元对另一个神经元的影响强度，使其兴奋（正权重）或抑制（负权重）。输出信号如果高于阈值，则神经元激活（对应于激活函数$f(\\sum_{i}w_ix_i+b)$）。\n\n1. 1 多层感知机\n\n   输入层  -> 隐藏层 -> 输出层\n\n   ![img](nn_cs231n_note/ccb56c1fb267bc632d6d88459eb14ace_hd.png)\n\n## 2. 常用激活函数\n\n|         | 函数                                    | 值域          | 导数 | 备注 |\n| ------- | --------------------------------------- | ------------- | ---- | ---- |\n| sigmoid | $ \\sigma(x)=\\frac{1}{1+e^{-x}} $        | [0,1]         | [0, 0.25]   | 分类概率 |\n| tanh    | $tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$ | [-1,1]        | [0, 1]     |      |\n| relu    | $relu(x)=max(0, x)$                     | $[0,+\\infty]$ |$f'_x=\\begin{cases}0, x<0\\\\1, x>0\\\\undefinded, x=0\\end{cases}$||\n#### sigmoid\n\n优点：\n\n1. 分类概率\n\n缺点\n\n1. 梯度容易饱和而丢失，激活函数在接近0,1时会饱和，如果权重过大，很容易失去梯度\n2. 函数不经过0，不以0点对称，在中间点0.5附近的的梯度较小（0.25），梯度总体较小，不利于梯度传播\n3. 激活值永远全为正（负），下一神经元的输入总是正数（负数），则反向传播过程中梯度更新呈z字型\n4. exp指数函数计算复杂\n\n![1560776135809](nn_cs231n_note/1560776135809.png)\n\n#### tanh\n\n$tanh(x)=2\\sigma(2x)-1$\n\n优点：\n\n1. 范围更大 [0, 1]\n2. 以 0 为中心点\n3. 中间部分梯度更大，有利于梯度传播\n\n缺点：\n\n1. 梯度饱和而丢失的情况仍然存在\n\n   ### relu\n\n优点\n\n1. 收敛速度更快（e.g. 6x than sigmoid/tanh)\n2. 计算简单\n3. 梯度不会饱和\n\n缺点\n\n1. 可能导致部分神经元“死掉”，永远不会被激活。输出值始终为负，激活值为0，梯度为0，反向传播不更新此神经元的梯度。降低学习率来降低神经元“死掉”的概率。\n\n   \n\n![677187e96671a4cac9c95352743b3806_hd](nn_cs231n_note/677187e96671a4cac9c95352743b3806_hd.png)\n\n\n\n![677187e96671a4cac9c95352743b3806_hd](nn_cs231n_note/677187e96671a4cac9c95352743b3806_hd.png)![img](nn_cs231n_note/42.png)\n\n#### Leaky Relu\n\n解决Relu死亡的问题\n\n$$f(x)=\\begin{cases}x, x>0\\\\\\alpha x, x<0\\end{cases}$$, $\\alpha=0.01​$\n\n#### ELU\n\n$$f(x)=\\begin{cases} x, x>0\\\\ \\alpha(e^x-1), x\\leq0\\end{cases}$$\n\n![1560776449074](nn_cs231n_note/1560776449074.png)\n\n#### Maxout\n\n$$max(w_{1}^Tx+b_1, w_{2}^Tx+b_2)$$\n\n## 3. 数据预处理\n\n### 归一化 Normalization\n\n```\n# 数据归一化\nX = X / np.std(X, axis=0)\n# 维度归一化\nX = X / np.std(X, axis=1)\n```\n\n![img](nn_cs231n_note/e743b6777775b1671c3b5503d7afbbc4_hd.png)\n\n ### PCA 白化（很少在深度学习中使用）\n\nPCA/白化。**左边**是二维的原始数据。**中间**：经过PCA操作的数据。可以看出数据首先是零中心的，然后变换到了数据协方差矩阵的基准轴上。这样就对数据进行了解相关（协方差矩阵变成对角阵）。**右边**：每个维度都被特征值调整数值范围，将数据协方差矩阵变为单位矩阵。从几何上看，就是对数据在各个方向上拉伸压缩，使之变成服从高斯分布的一个数据点分布。\n\n![img](nn_cs231n_note/aae11de6e6a29f50d46b9ea106fbb02a_hd.png)\n\n### CIFAR 数据PCA\n\nnx3072 维向量（图片32x32x3）,协方差矩阵：3072x3072\n\n![img](nn_cs231n_note/8608c06086fc196228f4dda78499a2d9_hd.png)\n\n1: 49张图片。2: 3072个特征值向量中的前144个。3: 49张PCA降维的图片（U.transpose()[:144,:]）。4: 白化后的数据。144个维度的方差都压缩到相同的数值范围（U.transpose()[:144,:]）。现在较低的频率（代表了大多数方差）可以忽略不计了，较高的频率（代表相对少的方差）就被夸大了。\n\n## 4. 权重初始化\n\n#### 小随机数初始化\n\n基于均值为0，标准差为1的高斯分布\n\n```\nW = 0.01 * np.random.randn(N, D)\n```\n\n#### 使用$\\frac{1}{\\sqrt{n}} $校准方差\n\n数据量增大，随机初始化的神经元输出数据分布的方差也增大\n\n```python\nW = np.random.randn(N, D) / np.sqrt(N)\n```\n\n#### He Normal\n\n网络中神经元的方差应该是$\\frac{2}{n}$\n\n当前的推荐是使用ReLU激活函数，并且使用**w = np.random.randn(n) \\* sqrt(2.0/n)**来进行权重初始化\n\n```python\nW = np.random.randn(n) * sqrt(2.0/n)\n```\n\n#### 偏置初始化 biases\n\n```\nb = np.zeros(n,)\n```\n\n## 正则化\n\n#### L1 正则\n\n\n\n#### L2正则\n\n\n\n## Dropout\n\n![img](nn_cs231n_note/63fcf4cc655cb04f21a37e86aca333cf_hd.png)\n\n1.  Bagging 集成模型，随机抽样神经网络的子集。多个共享参数的子网络组成。\n2. 增强单个神经元独立学习特征的能力，减少神经元之间的依赖\n3. 加性噪声\n\n```\n\"\"\"\n反向随机失活: 推荐实现方式.\n在训练的时候drop和调整数值范围，测试时不做任何事.\n\"\"\"\n\np = 0.5 # 激活神经元的概率. p值更高 = 随机失活更弱\n\ndef train_step(X):\n  # 3层neural network的前向传播\n  H1 = np.maximum(0, np.dot(W1, X) + b1)\n  #                  [0, 1]随机分布 P(rand(x)) < p = p\n  mask1 = (np.random.rand(*H1.shape) < p) / p # 第一个随机失活掩码. 注意/p!\n  H1 *= mask1 # drop!\n  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n  mask2 = (np.random.rand(*H2.shape) < p) / p # 第二个随机失活掩码. 注意/p!\n  H2 *= mask2 # drop!\n  out = np.dot(W3, H2) + b3\n\n  # 反向传播:计算梯度... (略)\n  # 进行参数更新... (略)\n\ndef predict(X):\n  # 前向传播时模型集成\n  H1 = np.maximum(0, np.dot(W1, X) + b1) # 不用数值范围调整了\n  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n  out = np.dot(W3, H2) + b3\n```\n\n## Batch Normalization\n\n批量归一化可以理解为在网络的每一层之前都做预处理，减少之前网络权重对数据的影响，保持每一层输出数据的分布（均值和标准差），使输出适应下一层网络，也使得每一层数据相对独立。\n\n![1560779540116](nn_cs231n_note/1560779540116.png)\n\n### Internal Co-variate Shift\n\nReference: [Batch Normalization原理与实战](<https://zhuanlan.zhihu.com/p/34879333>)\n\n随着训练的进行，网络中的参数也随着梯度下降在不停更新。一方面，当底层网络中参数发生微弱变化时，由于每一层中的线性变换与非线性激活映射，这些微弱变化随着网络层数的加深而被放大（类似蝴蝶效应）；另一方面，参数的变化导致每一层的输入分布会发生改变，进而下一层的网络需要不停地去适应这些分布变化，使得我们的模型训练变得困难。上述这一现象叫做Internal Covariate Shift。\n\n原作定义：在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程被称作Internal Covariate Shift。\n\n随着梯度下降的进行，每一层的参数$W^{[l]}$与$b^{[l]}$都会被更新，那么$Z^{[l]}$的分布也就发生了改变，进而$A^{[l]}$也同样出现分布的改变。而$A^{[l]}$作为第 $l+1$ 层的输入，意味着 $l+1$ 层需要去不停适应这种数据分布的变化，这一过程叫做 Interval Covariate Shift.\n\n#### 带来的问题：\n\n1. 上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低\n2. 网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度（sigmoid, tanh）。 $Z^{[l]}$会逐渐更新并变大，陷入梯度饱和区。可以通过Normalization 使得激活函数输入分布在一个稳定的空间来避免他们陷入梯度饱和区。\n\n#### 如何减缓 Interval Covariate Shift\n\n1. 白化。成本高，改变了网络每一层分布导致数据表达的特征信息丢失\n\n   - 使得输入特征分布具有相同的均值与方差。其中PCA白化保证了所有特征分布均值为0，方差为1\n   - 去除特征之间的相关性\n\n2. Batch Normalization   简化加改进版的白化\n\n   - 简化。让每个特征都有均值为0，方差为1的分布就OK。\n   - 白化操作减弱了网络中每一层输入数据表达能力，那我就再加个线性变换操作，让这些数据再能够尽可能恢复本身的表达能力就好了。\n\n   ![1560779531173](nn_cs231n_note/1560779531173.png)\n\nBN 引入了两个可学习的参数 $\\gamma$ 和 $\\beta$.。这两个参数的引入是为了恢复数据本身的表达能力，对规范后的数据进行线性变换，即 $y_i = \\gamma x_i + \\beta_i$。 特别的，当 $\\gamma^2=\\sigma ^2$（方差）, $\\beta = \\mu$ （均值）时，可以实现等价变换并且保留原始输入特征的分布信息。\n\n#### Batch Normalization 的作用\n\n1. 使得网络中每层输入数据的分布相对稳定，加快模型学习速度\n\n2. 使得模型对参数不那么敏感，减小初始化参数对模型学习的影响，可以选择更大的初始化值，学习率选择范围更大\n\n   当学习率设置太高时，会使得参数更新步伐过大，容易出现震荡和不收敛。但是使用BN的网络将不会受到参数数值大小的影响。BN抑制了参数微小变化随着网络层数加深被放大的问题，使得网络对参数大小的适应能力更强\n\n3. 缓解梯度消失的问题\n\n4. 正则化效果，mini-batch 的mean/variance 作为总体样本的抽样估计，引入随机噪声\n\n**BN通过将每一层网络的输入进行normalization，保证输入分布的均值与方差固定在一定范围内，减少了网络中的Internal Covariate Shift问题，并在一定程度上缓解了梯度消失，加速了模型收敛；并且BN使得网络对参数、激活函数更加具有鲁棒性，降低了神经网络模型训练和调参的复杂度；最后BN训练过程中由于使用mini-batch的mean/variance作为总体样本统计量估计，引入了随机噪声，在一定程度上对模型起到了正则化的效果。**\n\n#### backpropagation\n\nReference: [Understanding the backward pass through Batch Normalization Layer](<https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html>)","slug":"deep-learning/nn_cs231n_note","published":1,"updated":"2019-06-17T14:44:43.557Z","_id":"cjx0hdxnl0000n0fyvhy8xs6e","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"深度学习和神经网络-CS231n-Note\"><a href=\"#深度学习和神经网络-CS231n-Note\" class=\"headerlink\" title=\"深度学习和神经网络(CS231n Note)\"></a>深度学习和神经网络(CS231n Note)</h2><h2 id=\"1-神经元模型与数学模型（Neuron-Network-Unit）\"><a href=\"#1-神经元模型与数学模型（Neuron-Network-Unit）\" class=\"headerlink\" title=\"1. 神经元模型与数学模型（Neuron Network Unit）\"></a>1. 神经元模型与数学模型（Neuron Network Unit）</h2><p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/d0cbce2f2654b8e70fe201fec2982c7d_hd.png\" alt=\"img\"></p>\n<p>神经元（Neuron）通过树突（Dendrites）接收输入信号，沿着轴突（axon）产生输出信号。轴突在末端分叉，通过突触和其他神经元的树突相连。</p>\n<p>输入信号（$x_0,x_1,…,x_n$）传递到其他神经元的树突，基于突触的突触强度相乘（$w_0x_0,w_1x_1,…,w_nx_n$）。突触的强度（权重$w$）可以控制一个神经元对另一个神经元的影响强度，使其兴奋（正权重）或抑制（负权重）。输出信号如果高于阈值，则神经元激活（对应于激活函数$f(\\sum_{i}w_ix_i+b)$）。</p>\n<ol>\n<li><p>1 多层感知机</p>\n<p>输入层  -&gt; 隐藏层 -&gt; 输出层</p>\n<p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/ccb56c1fb267bc632d6d88459eb14ace_hd.png\" alt=\"img\"></p>\n</li>\n</ol>\n<h2 id=\"2-常用激活函数\"><a href=\"#2-常用激活函数\" class=\"headerlink\" title=\"2. 常用激活函数\"></a>2. 常用激活函数</h2><table>\n<thead>\n<tr>\n<th></th>\n<th>函数</th>\n<th>值域</th>\n<th>导数</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>sigmoid</td>\n<td>$ \\sigma(x)=\\frac{1}{1+e^{-x}} $</td>\n<td>[0,1]</td>\n<td>[0, 0.25]</td>\n<td>分类概率</td>\n</tr>\n<tr>\n<td>tanh</td>\n<td>$tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$</td>\n<td>[-1,1]</td>\n<td>[0, 1]</td>\n<td></td>\n</tr>\n<tr>\n<td>relu</td>\n<td>$relu(x)=max(0, x)$</td>\n<td>$[0,+\\infty]$</td>\n<td>$f’_x=\\begin{cases}0, x&lt;0\\1, x&gt;0\\undefinded, x=0\\end{cases}$</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"sigmoid\"><a href=\"#sigmoid\" class=\"headerlink\" title=\"sigmoid\"></a>sigmoid</h4><p>优点：</p>\n<ol>\n<li>分类概率</li>\n</ol>\n<p>缺点</p>\n<ol>\n<li>梯度容易饱和而丢失，激活函数在接近0,1时会饱和，如果权重过大，很容易失去梯度</li>\n<li>函数不经过0，不以0点对称，在中间点0.5附近的的梯度较小（0.25），梯度总体较小，不利于梯度传播</li>\n<li>激活值永远全为正（负），下一神经元的输入总是正数（负数），则反向传播过程中梯度更新呈z字型</li>\n<li>exp指数函数计算复杂</li>\n</ol>\n<p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/1560776135809.png\" alt=\"1560776135809\"></p>\n<h4 id=\"tanh\"><a href=\"#tanh\" class=\"headerlink\" title=\"tanh\"></a>tanh</h4><p>$tanh(x)=2\\sigma(2x)-1$</p>\n<p>优点：</p>\n<ol>\n<li>范围更大 [0, 1]</li>\n<li>以 0 为中心点</li>\n<li>中间部分梯度更大，有利于梯度传播</li>\n</ol>\n<p>缺点：</p>\n<ol>\n<li><p>梯度饱和而丢失的情况仍然存在</p>\n<h3 id=\"relu\"><a href=\"#relu\" class=\"headerlink\" title=\"relu\"></a>relu</h3></li>\n</ol>\n<p>优点</p>\n<ol>\n<li>收敛速度更快（e.g. 6x than sigmoid/tanh)</li>\n<li>计算简单</li>\n<li>梯度不会饱和</li>\n</ol>\n<p>缺点</p>\n<ol>\n<li>可能导致部分神经元“死掉”，永远不会被激活。输出值始终为负，激活值为0，梯度为0，反向传播不更新此神经元的梯度。降低学习率来降低神经元“死掉”的概率。</li>\n</ol>\n<p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/677187e96671a4cac9c95352743b3806_hd.png\" alt=\"677187e96671a4cac9c95352743b3806_hd\"></p>\n<p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/677187e96671a4cac9c95352743b3806_hd.png\" alt=\"677187e96671a4cac9c95352743b3806_hd\"><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/42.png\" alt=\"img\"></p>\n<h4 id=\"Leaky-Relu\"><a href=\"#Leaky-Relu\" class=\"headerlink\" title=\"Leaky Relu\"></a>Leaky Relu</h4><p>解决Relu死亡的问题</p>\n<p>$$f(x)=\\begin{cases}x, x&gt;0\\\\alpha x, x&lt;0\\end{cases}$$, $\\alpha=0.01​$</p>\n<h4 id=\"ELU\"><a href=\"#ELU\" class=\"headerlink\" title=\"ELU\"></a>ELU</h4><p>$$f(x)=\\begin{cases} x, x&gt;0\\ \\alpha(e^x-1), x\\leq0\\end{cases}$$</p>\n<p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/1560776449074.png\" alt=\"1560776449074\"></p>\n<h4 id=\"Maxout\"><a href=\"#Maxout\" class=\"headerlink\" title=\"Maxout\"></a>Maxout</h4><p>$$max(w_{1}^Tx+b_1, w_{2}^Tx+b_2)$$</p>\n<h2 id=\"3-数据预处理\"><a href=\"#3-数据预处理\" class=\"headerlink\" title=\"3. 数据预处理\"></a>3. 数据预处理</h2><h3 id=\"归一化-Normalization\"><a href=\"#归一化-Normalization\" class=\"headerlink\" title=\"归一化 Normalization\"></a>归一化 Normalization</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 数据归一化</span><br><span class=\"line\">X = X / np.std(X, axis=0)</span><br><span class=\"line\"># 维度归一化</span><br><span class=\"line\">X = X / np.std(X, axis=1)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/e743b6777775b1671c3b5503d7afbbc4_hd.png\" alt=\"img\"></p>\n<h3 id=\"PCA-白化（很少在深度学习中使用）\"><a href=\"#PCA-白化（很少在深度学习中使用）\" class=\"headerlink\" title=\"PCA 白化（很少在深度学习中使用）\"></a>PCA 白化（很少在深度学习中使用）</h3><p>PCA/白化。<strong>左边</strong>是二维的原始数据。<strong>中间</strong>：经过PCA操作的数据。可以看出数据首先是零中心的，然后变换到了数据协方差矩阵的基准轴上。这样就对数据进行了解相关（协方差矩阵变成对角阵）。<strong>右边</strong>：每个维度都被特征值调整数值范围，将数据协方差矩阵变为单位矩阵。从几何上看，就是对数据在各个方向上拉伸压缩，使之变成服从高斯分布的一个数据点分布。</p>\n<p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/aae11de6e6a29f50d46b9ea106fbb02a_hd.png\" alt=\"img\"></p>\n<h3 id=\"CIFAR-数据PCA\"><a href=\"#CIFAR-数据PCA\" class=\"headerlink\" title=\"CIFAR 数据PCA\"></a>CIFAR 数据PCA</h3><p>nx3072 维向量（图片32x32x3）,协方差矩阵：3072x3072</p>\n<p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/8608c06086fc196228f4dda78499a2d9_hd.png\" alt=\"img\"></p>\n<p>1: 49张图片。2: 3072个特征值向量中的前144个。3: 49张PCA降维的图片（U.transpose()[:144,:]）。4: 白化后的数据。144个维度的方差都压缩到相同的数值范围（U.transpose()[:144,:]）。现在较低的频率（代表了大多数方差）可以忽略不计了，较高的频率（代表相对少的方差）就被夸大了。</p>\n<h2 id=\"4-权重初始化\"><a href=\"#4-权重初始化\" class=\"headerlink\" title=\"4. 权重初始化\"></a>4. 权重初始化</h2><h4 id=\"小随机数初始化\"><a href=\"#小随机数初始化\" class=\"headerlink\" title=\"小随机数初始化\"></a>小随机数初始化</h4><p>基于均值为0，标准差为1的高斯分布</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">W = 0.01 * np.random.randn(N, D)</span><br></pre></td></tr></table></figure>\n<h4 id=\"使用-frac-1-sqrt-n-校准方差\"><a href=\"#使用-frac-1-sqrt-n-校准方差\" class=\"headerlink\" title=\"使用$\\frac{1}{\\sqrt{n}} $校准方差\"></a>使用$\\frac{1}{\\sqrt{n}} $校准方差</h4><p>数据量增大，随机初始化的神经元输出数据分布的方差也增大</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">W = np.random.randn(N, D) / np.sqrt(N)</span><br></pre></td></tr></table></figure>\n<h4 id=\"He-Normal\"><a href=\"#He-Normal\" class=\"headerlink\" title=\"He Normal\"></a>He Normal</h4><p>网络中神经元的方差应该是$\\frac{2}{n}$</p>\n<p>当前的推荐是使用ReLU激活函数，并且使用<strong>w = np.random.randn(n) * sqrt(2.0/n)</strong>来进行权重初始化</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">W = np.random.randn(n) * sqrt(<span class=\"number\">2.0</span>/n)</span><br></pre></td></tr></table></figure>\n<h4 id=\"偏置初始化-biases\"><a href=\"#偏置初始化-biases\" class=\"headerlink\" title=\"偏置初始化 biases\"></a>偏置初始化 biases</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b = np.zeros(n,)</span><br></pre></td></tr></table></figure>\n<h2 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a>正则化</h2><h4 id=\"L1-正则\"><a href=\"#L1-正则\" class=\"headerlink\" title=\"L1 正则\"></a>L1 正则</h4><h4 id=\"L2正则\"><a href=\"#L2正则\" class=\"headerlink\" title=\"L2正则\"></a>L2正则</h4><h2 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h2><p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/63fcf4cc655cb04f21a37e86aca333cf_hd.png\" alt=\"img\"></p>\n<ol>\n<li>Bagging 集成模型，随机抽样神经网络的子集。多个共享参数的子网络组成。</li>\n<li>增强单个神经元独立学习特征的能力，减少神经元之间的依赖</li>\n<li>加性噪声</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;&quot;&quot;</span><br><span class=\"line\">反向随机失活: 推荐实现方式.</span><br><span class=\"line\">在训练的时候drop和调整数值范围，测试时不做任何事.</span><br><span class=\"line\">&quot;&quot;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">p = 0.5 # 激活神经元的概率. p值更高 = 随机失活更弱</span><br><span class=\"line\"></span><br><span class=\"line\">def train_step(X):</span><br><span class=\"line\">  # 3层neural network的前向传播</span><br><span class=\"line\">  H1 = np.maximum(0, np.dot(W1, X) + b1)</span><br><span class=\"line\">  #                  [0, 1]随机分布 P(rand(x)) &lt; p = p</span><br><span class=\"line\">  mask1 = (np.random.rand(*H1.shape) &lt; p) / p # 第一个随机失活掩码. 注意/p!</span><br><span class=\"line\">  H1 *= mask1 # drop!</span><br><span class=\"line\">  H2 = np.maximum(0, np.dot(W2, H1) + b2)</span><br><span class=\"line\">  mask2 = (np.random.rand(*H2.shape) &lt; p) / p # 第二个随机失活掩码. 注意/p!</span><br><span class=\"line\">  H2 *= mask2 # drop!</span><br><span class=\"line\">  out = np.dot(W3, H2) + b3</span><br><span class=\"line\"></span><br><span class=\"line\">  # 反向传播:计算梯度... (略)</span><br><span class=\"line\">  # 进行参数更新... (略)</span><br><span class=\"line\"></span><br><span class=\"line\">def predict(X):</span><br><span class=\"line\">  # 前向传播时模型集成</span><br><span class=\"line\">  H1 = np.maximum(0, np.dot(W1, X) + b1) # 不用数值范围调整了</span><br><span class=\"line\">  H2 = np.maximum(0, np.dot(W2, H1) + b2)</span><br><span class=\"line\">  out = np.dot(W3, H2) + b3</span><br></pre></td></tr></table></figure>\n<h2 id=\"Batch-Normalization\"><a href=\"#Batch-Normalization\" class=\"headerlink\" title=\"Batch Normalization\"></a>Batch Normalization</h2><p>批量归一化可以理解为在网络的每一层之前都做预处理，减少之前网络权重对数据的影响，保持每一层输出数据的分布（均值和标准差），使输出适应下一层网络，也使得每一层数据相对独立。</p>\n<p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/1560779540116.png\" alt=\"1560779540116\"></p>\n<h3 id=\"Internal-Co-variate-Shift\"><a href=\"#Internal-Co-variate-Shift\" class=\"headerlink\" title=\"Internal Co-variate Shift\"></a>Internal Co-variate Shift</h3><p>Reference: <a href=\"https://zhuanlan.zhihu.com/p/34879333\" target=\"_blank\" rel=\"noopener\">Batch Normalization原理与实战</a></p>\n<p>随着训练的进行，网络中的参数也随着梯度下降在不停更新。一方面，当底层网络中参数发生微弱变化时，由于每一层中的线性变换与非线性激活映射，这些微弱变化随着网络层数的加深而被放大（类似蝴蝶效应）；另一方面，参数的变化导致每一层的输入分布会发生改变，进而下一层的网络需要不停地去适应这些分布变化，使得我们的模型训练变得困难。上述这一现象叫做Internal Covariate Shift。</p>\n<p>原作定义：在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程被称作Internal Covariate Shift。</p>\n<p>随着梯度下降的进行，每一层的参数$W^{[l]}$与$b^{[l]}$都会被更新，那么$Z^{[l]}$的分布也就发生了改变，进而$A^{[l]}$也同样出现分布的改变。而$A^{[l]}$作为第 $l+1$ 层的输入，意味着 $l+1$ 层需要去不停适应这种数据分布的变化，这一过程叫做 Interval Covariate Shift.</p>\n<h4 id=\"带来的问题：\"><a href=\"#带来的问题：\" class=\"headerlink\" title=\"带来的问题：\"></a>带来的问题：</h4><ol>\n<li>上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低</li>\n<li>网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度（sigmoid, tanh）。 $Z^{[l]}$会逐渐更新并变大，陷入梯度饱和区。可以通过Normalization 使得激活函数输入分布在一个稳定的空间来避免他们陷入梯度饱和区。</li>\n</ol>\n<h4 id=\"如何减缓-Interval-Covariate-Shift\"><a href=\"#如何减缓-Interval-Covariate-Shift\" class=\"headerlink\" title=\"如何减缓 Interval Covariate Shift\"></a>如何减缓 Interval Covariate Shift</h4><ol>\n<li><p>白化。成本高，改变了网络每一层分布导致数据表达的特征信息丢失</p>\n<ul>\n<li>使得输入特征分布具有相同的均值与方差。其中PCA白化保证了所有特征分布均值为0，方差为1</li>\n<li>去除特征之间的相关性</li>\n</ul>\n</li>\n<li><p>Batch Normalization   简化加改进版的白化</p>\n<ul>\n<li>简化。让每个特征都有均值为0，方差为1的分布就OK。</li>\n<li>白化操作减弱了网络中每一层输入数据表达能力，那我就再加个线性变换操作，让这些数据再能够尽可能恢复本身的表达能力就好了。</li>\n</ul>\n<p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/1560779531173.png\" alt=\"1560779531173\"></p>\n</li>\n</ol>\n<p>BN 引入了两个可学习的参数 $\\gamma$ 和 $\\beta$.。这两个参数的引入是为了恢复数据本身的表达能力，对规范后的数据进行线性变换，即 $y_i = \\gamma x_i + \\beta_i$。 特别的，当 $\\gamma^2=\\sigma ^2$（方差）, $\\beta = \\mu$ （均值）时，可以实现等价变换并且保留原始输入特征的分布信息。</p>\n<h4 id=\"Batch-Normalization-的作用\"><a href=\"#Batch-Normalization-的作用\" class=\"headerlink\" title=\"Batch Normalization 的作用\"></a>Batch Normalization 的作用</h4><ol>\n<li><p>使得网络中每层输入数据的分布相对稳定，加快模型学习速度</p>\n</li>\n<li><p>使得模型对参数不那么敏感，减小初始化参数对模型学习的影响，可以选择更大的初始化值，学习率选择范围更大</p>\n<p>当学习率设置太高时，会使得参数更新步伐过大，容易出现震荡和不收敛。但是使用BN的网络将不会受到参数数值大小的影响。BN抑制了参数微小变化随着网络层数加深被放大的问题，使得网络对参数大小的适应能力更强</p>\n</li>\n<li><p>缓解梯度消失的问题</p>\n</li>\n<li><p>正则化效果，mini-batch 的mean/variance 作为总体样本的抽样估计，引入随机噪声</p>\n</li>\n</ol>\n<p><strong>BN通过将每一层网络的输入进行normalization，保证输入分布的均值与方差固定在一定范围内，减少了网络中的Internal Covariate Shift问题，并在一定程度上缓解了梯度消失，加速了模型收敛；并且BN使得网络对参数、激活函数更加具有鲁棒性，降低了神经网络模型训练和调参的复杂度；最后BN训练过程中由于使用mini-batch的mean/variance作为总体样本统计量估计，引入了随机噪声，在一定程度上对模型起到了正则化的效果。</strong></p>\n<h4 id=\"backpropagation\"><a href=\"#backpropagation\" class=\"headerlink\" title=\"backpropagation\"></a>backpropagation</h4><p>Reference: <a href=\"https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\" target=\"_blank\" rel=\"noopener\">Understanding the backward pass through Batch Normalization Layer</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"深度学习和神经网络-CS231n-Note\"><a href=\"#深度学习和神经网络-CS231n-Note\" class=\"headerlink\" title=\"深度学习和神经网络(CS231n Note)\"></a>深度学习和神经网络(CS231n Note)</h2><h2 id=\"1-神经元模型与数学模型（Neuron-Network-Unit）\"><a href=\"#1-神经元模型与数学模型（Neuron-Network-Unit）\" class=\"headerlink\" title=\"1. 神经元模型与数学模型（Neuron Network Unit）\"></a>1. 神经元模型与数学模型（Neuron Network Unit）</h2><p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/d0cbce2f2654b8e70fe201fec2982c7d_hd.png\" alt=\"img\"></p>\n<p>神经元（Neuron）通过树突（Dendrites）接收输入信号，沿着轴突（axon）产生输出信号。轴突在末端分叉，通过突触和其他神经元的树突相连。</p>\n<p>输入信号（$x_0,x_1,…,x_n$）传递到其他神经元的树突，基于突触的突触强度相乘（$w_0x_0,w_1x_1,…,w_nx_n$）。突触的强度（权重$w$）可以控制一个神经元对另一个神经元的影响强度，使其兴奋（正权重）或抑制（负权重）。输出信号如果高于阈值，则神经元激活（对应于激活函数$f(\\sum_{i}w_ix_i+b)$）。</p>\n<ol>\n<li><p>1 多层感知机</p>\n<p>输入层  -&gt; 隐藏层 -&gt; 输出层</p>\n<p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/ccb56c1fb267bc632d6d88459eb14ace_hd.png\" alt=\"img\"></p>\n</li>\n</ol>\n<h2 id=\"2-常用激活函数\"><a href=\"#2-常用激活函数\" class=\"headerlink\" title=\"2. 常用激活函数\"></a>2. 常用激活函数</h2><table>\n<thead>\n<tr>\n<th></th>\n<th>函数</th>\n<th>值域</th>\n<th>导数</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>sigmoid</td>\n<td>$ \\sigma(x)=\\frac{1}{1+e^{-x}} $</td>\n<td>[0,1]</td>\n<td>[0, 0.25]</td>\n<td>分类概率</td>\n</tr>\n<tr>\n<td>tanh</td>\n<td>$tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$</td>\n<td>[-1,1]</td>\n<td>[0, 1]</td>\n<td></td>\n</tr>\n<tr>\n<td>relu</td>\n<td>$relu(x)=max(0, x)$</td>\n<td>$[0,+\\infty]$</td>\n<td>$f’_x=\\begin{cases}0, x&lt;0\\1, x&gt;0\\undefinded, x=0\\end{cases}$</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"sigmoid\"><a href=\"#sigmoid\" class=\"headerlink\" title=\"sigmoid\"></a>sigmoid</h4><p>优点：</p>\n<ol>\n<li>分类概率</li>\n</ol>\n<p>缺点</p>\n<ol>\n<li>梯度容易饱和而丢失，激活函数在接近0,1时会饱和，如果权重过大，很容易失去梯度</li>\n<li>函数不经过0，不以0点对称，在中间点0.5附近的的梯度较小（0.25），梯度总体较小，不利于梯度传播</li>\n<li>激活值永远全为正（负），下一神经元的输入总是正数（负数），则反向传播过程中梯度更新呈z字型</li>\n<li>exp指数函数计算复杂</li>\n</ol>\n<p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/1560776135809.png\" alt=\"1560776135809\"></p>\n<h4 id=\"tanh\"><a href=\"#tanh\" class=\"headerlink\" title=\"tanh\"></a>tanh</h4><p>$tanh(x)=2\\sigma(2x)-1$</p>\n<p>优点：</p>\n<ol>\n<li>范围更大 [0, 1]</li>\n<li>以 0 为中心点</li>\n<li>中间部分梯度更大，有利于梯度传播</li>\n</ol>\n<p>缺点：</p>\n<ol>\n<li><p>梯度饱和而丢失的情况仍然存在</p>\n<h3 id=\"relu\"><a href=\"#relu\" class=\"headerlink\" title=\"relu\"></a>relu</h3></li>\n</ol>\n<p>优点</p>\n<ol>\n<li>收敛速度更快（e.g. 6x than sigmoid/tanh)</li>\n<li>计算简单</li>\n<li>梯度不会饱和</li>\n</ol>\n<p>缺点</p>\n<ol>\n<li>可能导致部分神经元“死掉”，永远不会被激活。输出值始终为负，激活值为0，梯度为0，反向传播不更新此神经元的梯度。降低学习率来降低神经元“死掉”的概率。</li>\n</ol>\n<p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/677187e96671a4cac9c95352743b3806_hd.png\" alt=\"677187e96671a4cac9c95352743b3806_hd\"></p>\n<p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/677187e96671a4cac9c95352743b3806_hd.png\" alt=\"677187e96671a4cac9c95352743b3806_hd\"><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/42.png\" alt=\"img\"></p>\n<h4 id=\"Leaky-Relu\"><a href=\"#Leaky-Relu\" class=\"headerlink\" title=\"Leaky Relu\"></a>Leaky Relu</h4><p>解决Relu死亡的问题</p>\n<p>$$f(x)=\\begin{cases}x, x&gt;0\\\\alpha x, x&lt;0\\end{cases}$$, $\\alpha=0.01​$</p>\n<h4 id=\"ELU\"><a href=\"#ELU\" class=\"headerlink\" title=\"ELU\"></a>ELU</h4><p>$$f(x)=\\begin{cases} x, x&gt;0\\ \\alpha(e^x-1), x\\leq0\\end{cases}$$</p>\n<p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/1560776449074.png\" alt=\"1560776449074\"></p>\n<h4 id=\"Maxout\"><a href=\"#Maxout\" class=\"headerlink\" title=\"Maxout\"></a>Maxout</h4><p>$$max(w_{1}^Tx+b_1, w_{2}^Tx+b_2)$$</p>\n<h2 id=\"3-数据预处理\"><a href=\"#3-数据预处理\" class=\"headerlink\" title=\"3. 数据预处理\"></a>3. 数据预处理</h2><h3 id=\"归一化-Normalization\"><a href=\"#归一化-Normalization\" class=\"headerlink\" title=\"归一化 Normalization\"></a>归一化 Normalization</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 数据归一化</span><br><span class=\"line\">X = X / np.std(X, axis=0)</span><br><span class=\"line\"># 维度归一化</span><br><span class=\"line\">X = X / np.std(X, axis=1)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/e743b6777775b1671c3b5503d7afbbc4_hd.png\" alt=\"img\"></p>\n<h3 id=\"PCA-白化（很少在深度学习中使用）\"><a href=\"#PCA-白化（很少在深度学习中使用）\" class=\"headerlink\" title=\"PCA 白化（很少在深度学习中使用）\"></a>PCA 白化（很少在深度学习中使用）</h3><p>PCA/白化。<strong>左边</strong>是二维的原始数据。<strong>中间</strong>：经过PCA操作的数据。可以看出数据首先是零中心的，然后变换到了数据协方差矩阵的基准轴上。这样就对数据进行了解相关（协方差矩阵变成对角阵）。<strong>右边</strong>：每个维度都被特征值调整数值范围，将数据协方差矩阵变为单位矩阵。从几何上看，就是对数据在各个方向上拉伸压缩，使之变成服从高斯分布的一个数据点分布。</p>\n<p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/aae11de6e6a29f50d46b9ea106fbb02a_hd.png\" alt=\"img\"></p>\n<h3 id=\"CIFAR-数据PCA\"><a href=\"#CIFAR-数据PCA\" class=\"headerlink\" title=\"CIFAR 数据PCA\"></a>CIFAR 数据PCA</h3><p>nx3072 维向量（图片32x32x3）,协方差矩阵：3072x3072</p>\n<p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/8608c06086fc196228f4dda78499a2d9_hd.png\" alt=\"img\"></p>\n<p>1: 49张图片。2: 3072个特征值向量中的前144个。3: 49张PCA降维的图片（U.transpose()[:144,:]）。4: 白化后的数据。144个维度的方差都压缩到相同的数值范围（U.transpose()[:144,:]）。现在较低的频率（代表了大多数方差）可以忽略不计了，较高的频率（代表相对少的方差）就被夸大了。</p>\n<h2 id=\"4-权重初始化\"><a href=\"#4-权重初始化\" class=\"headerlink\" title=\"4. 权重初始化\"></a>4. 权重初始化</h2><h4 id=\"小随机数初始化\"><a href=\"#小随机数初始化\" class=\"headerlink\" title=\"小随机数初始化\"></a>小随机数初始化</h4><p>基于均值为0，标准差为1的高斯分布</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">W = 0.01 * np.random.randn(N, D)</span><br></pre></td></tr></table></figure>\n<h4 id=\"使用-frac-1-sqrt-n-校准方差\"><a href=\"#使用-frac-1-sqrt-n-校准方差\" class=\"headerlink\" title=\"使用$\\frac{1}{\\sqrt{n}} $校准方差\"></a>使用$\\frac{1}{\\sqrt{n}} $校准方差</h4><p>数据量增大，随机初始化的神经元输出数据分布的方差也增大</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">W = np.random.randn(N, D) / np.sqrt(N)</span><br></pre></td></tr></table></figure>\n<h4 id=\"He-Normal\"><a href=\"#He-Normal\" class=\"headerlink\" title=\"He Normal\"></a>He Normal</h4><p>网络中神经元的方差应该是$\\frac{2}{n}$</p>\n<p>当前的推荐是使用ReLU激活函数，并且使用<strong>w = np.random.randn(n) * sqrt(2.0/n)</strong>来进行权重初始化</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">W = np.random.randn(n) * sqrt(<span class=\"number\">2.0</span>/n)</span><br></pre></td></tr></table></figure>\n<h4 id=\"偏置初始化-biases\"><a href=\"#偏置初始化-biases\" class=\"headerlink\" title=\"偏置初始化 biases\"></a>偏置初始化 biases</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b = np.zeros(n,)</span><br></pre></td></tr></table></figure>\n<h2 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a>正则化</h2><h4 id=\"L1-正则\"><a href=\"#L1-正则\" class=\"headerlink\" title=\"L1 正则\"></a>L1 正则</h4><h4 id=\"L2正则\"><a href=\"#L2正则\" class=\"headerlink\" title=\"L2正则\"></a>L2正则</h4><h2 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h2><p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/63fcf4cc655cb04f21a37e86aca333cf_hd.png\" alt=\"img\"></p>\n<ol>\n<li>Bagging 集成模型，随机抽样神经网络的子集。多个共享参数的子网络组成。</li>\n<li>增强单个神经元独立学习特征的能力，减少神经元之间的依赖</li>\n<li>加性噪声</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;&quot;&quot;</span><br><span class=\"line\">反向随机失活: 推荐实现方式.</span><br><span class=\"line\">在训练的时候drop和调整数值范围，测试时不做任何事.</span><br><span class=\"line\">&quot;&quot;&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">p = 0.5 # 激活神经元的概率. p值更高 = 随机失活更弱</span><br><span class=\"line\"></span><br><span class=\"line\">def train_step(X):</span><br><span class=\"line\">  # 3层neural network的前向传播</span><br><span class=\"line\">  H1 = np.maximum(0, np.dot(W1, X) + b1)</span><br><span class=\"line\">  #                  [0, 1]随机分布 P(rand(x)) &lt; p = p</span><br><span class=\"line\">  mask1 = (np.random.rand(*H1.shape) &lt; p) / p # 第一个随机失活掩码. 注意/p!</span><br><span class=\"line\">  H1 *= mask1 # drop!</span><br><span class=\"line\">  H2 = np.maximum(0, np.dot(W2, H1) + b2)</span><br><span class=\"line\">  mask2 = (np.random.rand(*H2.shape) &lt; p) / p # 第二个随机失活掩码. 注意/p!</span><br><span class=\"line\">  H2 *= mask2 # drop!</span><br><span class=\"line\">  out = np.dot(W3, H2) + b3</span><br><span class=\"line\"></span><br><span class=\"line\">  # 反向传播:计算梯度... (略)</span><br><span class=\"line\">  # 进行参数更新... (略)</span><br><span class=\"line\"></span><br><span class=\"line\">def predict(X):</span><br><span class=\"line\">  # 前向传播时模型集成</span><br><span class=\"line\">  H1 = np.maximum(0, np.dot(W1, X) + b1) # 不用数值范围调整了</span><br><span class=\"line\">  H2 = np.maximum(0, np.dot(W2, H1) + b2)</span><br><span class=\"line\">  out = np.dot(W3, H2) + b3</span><br></pre></td></tr></table></figure>\n<h2 id=\"Batch-Normalization\"><a href=\"#Batch-Normalization\" class=\"headerlink\" title=\"Batch Normalization\"></a>Batch Normalization</h2><p>批量归一化可以理解为在网络的每一层之前都做预处理，减少之前网络权重对数据的影响，保持每一层输出数据的分布（均值和标准差），使输出适应下一层网络，也使得每一层数据相对独立。</p>\n<p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/1560779540116.png\" alt=\"1560779540116\"></p>\n<h3 id=\"Internal-Co-variate-Shift\"><a href=\"#Internal-Co-variate-Shift\" class=\"headerlink\" title=\"Internal Co-variate Shift\"></a>Internal Co-variate Shift</h3><p>Reference: <a href=\"https://zhuanlan.zhihu.com/p/34879333\" target=\"_blank\" rel=\"noopener\">Batch Normalization原理与实战</a></p>\n<p>随着训练的进行，网络中的参数也随着梯度下降在不停更新。一方面，当底层网络中参数发生微弱变化时，由于每一层中的线性变换与非线性激活映射，这些微弱变化随着网络层数的加深而被放大（类似蝴蝶效应）；另一方面，参数的变化导致每一层的输入分布会发生改变，进而下一层的网络需要不停地去适应这些分布变化，使得我们的模型训练变得困难。上述这一现象叫做Internal Covariate Shift。</p>\n<p>原作定义：在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程被称作Internal Covariate Shift。</p>\n<p>随着梯度下降的进行，每一层的参数$W^{[l]}$与$b^{[l]}$都会被更新，那么$Z^{[l]}$的分布也就发生了改变，进而$A^{[l]}$也同样出现分布的改变。而$A^{[l]}$作为第 $l+1$ 层的输入，意味着 $l+1$ 层需要去不停适应这种数据分布的变化，这一过程叫做 Interval Covariate Shift.</p>\n<h4 id=\"带来的问题：\"><a href=\"#带来的问题：\" class=\"headerlink\" title=\"带来的问题：\"></a>带来的问题：</h4><ol>\n<li>上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低</li>\n<li>网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度（sigmoid, tanh）。 $Z^{[l]}$会逐渐更新并变大，陷入梯度饱和区。可以通过Normalization 使得激活函数输入分布在一个稳定的空间来避免他们陷入梯度饱和区。</li>\n</ol>\n<h4 id=\"如何减缓-Interval-Covariate-Shift\"><a href=\"#如何减缓-Interval-Covariate-Shift\" class=\"headerlink\" title=\"如何减缓 Interval Covariate Shift\"></a>如何减缓 Interval Covariate Shift</h4><ol>\n<li><p>白化。成本高，改变了网络每一层分布导致数据表达的特征信息丢失</p>\n<ul>\n<li>使得输入特征分布具有相同的均值与方差。其中PCA白化保证了所有特征分布均值为0，方差为1</li>\n<li>去除特征之间的相关性</li>\n</ul>\n</li>\n<li><p>Batch Normalization   简化加改进版的白化</p>\n<ul>\n<li>简化。让每个特征都有均值为0，方差为1的分布就OK。</li>\n<li>白化操作减弱了网络中每一层输入数据表达能力，那我就再加个线性变换操作，让这些数据再能够尽可能恢复本身的表达能力就好了。</li>\n</ul>\n<p><img src=\"/2019/06/17/deep-learning/nn_cs231n_note/1560779531173.png\" alt=\"1560779531173\"></p>\n</li>\n</ol>\n<p>BN 引入了两个可学习的参数 $\\gamma$ 和 $\\beta$.。这两个参数的引入是为了恢复数据本身的表达能力，对规范后的数据进行线性变换，即 $y_i = \\gamma x_i + \\beta_i$。 特别的，当 $\\gamma^2=\\sigma ^2$（方差）, $\\beta = \\mu$ （均值）时，可以实现等价变换并且保留原始输入特征的分布信息。</p>\n<h4 id=\"Batch-Normalization-的作用\"><a href=\"#Batch-Normalization-的作用\" class=\"headerlink\" title=\"Batch Normalization 的作用\"></a>Batch Normalization 的作用</h4><ol>\n<li><p>使得网络中每层输入数据的分布相对稳定，加快模型学习速度</p>\n</li>\n<li><p>使得模型对参数不那么敏感，减小初始化参数对模型学习的影响，可以选择更大的初始化值，学习率选择范围更大</p>\n<p>当学习率设置太高时，会使得参数更新步伐过大，容易出现震荡和不收敛。但是使用BN的网络将不会受到参数数值大小的影响。BN抑制了参数微小变化随着网络层数加深被放大的问题，使得网络对参数大小的适应能力更强</p>\n</li>\n<li><p>缓解梯度消失的问题</p>\n</li>\n<li><p>正则化效果，mini-batch 的mean/variance 作为总体样本的抽样估计，引入随机噪声</p>\n</li>\n</ol>\n<p><strong>BN通过将每一层网络的输入进行normalization，保证输入分布的均值与方差固定在一定范围内，减少了网络中的Internal Covariate Shift问题，并在一定程度上缓解了梯度消失，加速了模型收敛；并且BN使得网络对参数、激活函数更加具有鲁棒性，降低了神经网络模型训练和调参的复杂度；最后BN训练过程中由于使用mini-batch的mean/variance作为总体样本统计量估计，引入了随机噪声，在一定程度上对模型起到了正则化的效果。</strong></p>\n<h4 id=\"backpropagation\"><a href=\"#backpropagation\" class=\"headerlink\" title=\"backpropagation\"></a>backpropagation</h4><p>Reference: <a href=\"https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\" target=\"_blank\" rel=\"noopener\">Understanding the backward pass through Batch Normalization Layer</a></p>\n"},{"title":"Decistion Tree","date":"2019-01-31T12:33:05.000Z","mathjax":true,"_content":"\n## Part 4. 决策树\n\n--------\n\n\n\nDecision Tree　树形结构，根据损失函数最小化原则建立决策树模型\n\n![1548939696690](decision-tree/tree_basic.png)\n\nif-then规则集合，每一条路径构建一条规则\n\n- 内部节点：特征/属性，规则的条件\n\n- 叶节点：　类，规则的结论\n\n条件概率分布\n\n将特征空间划分为互不相交的单元（cell）或区域（region）\n\n$P(Y=+1|X=c)>0.5$, 单元cell　属于正类\n\n\n\n![1548940003249](decision-tree/feature-space.png)\n\n![1548940132559](decision-tree/1548940132559.png)\n\n![1548940093157](decision-tree/1548940093157.png)\n\n\n\n","source":"_posts/machine-learning/decision-tree.md","raw":"---\ntitle: Decistion Tree\ndate: 2019-01-31 20:33:05\nmathjax: true\ncategories:\n  - ml\ntag: \n  - ml\n  - hexo-asset-image\n---\n\n## Part 4. 决策树\n\n--------\n\n\n\nDecision Tree　树形结构，根据损失函数最小化原则建立决策树模型\n\n![1548939696690](decision-tree/tree_basic.png)\n\nif-then规则集合，每一条路径构建一条规则\n\n- 内部节点：特征/属性，规则的条件\n\n- 叶节点：　类，规则的结论\n\n条件概率分布\n\n将特征空间划分为互不相交的单元（cell）或区域（region）\n\n$P(Y=+1|X=c)>0.5$, 单元cell　属于正类\n\n\n\n![1548940003249](decision-tree/feature-space.png)\n\n![1548940132559](decision-tree/1548940132559.png)\n\n![1548940093157](decision-tree/1548940093157.png)\n\n\n\n","slug":"machine-learning/decision-tree","published":1,"updated":"2019-06-17T14:36:46.860Z","_id":"cjx0hdxnt0001n0fy90pyg85f","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"Part-4-决策树\"><a href=\"#Part-4-决策树\" class=\"headerlink\" title=\"Part 4. 决策树\"></a>Part 4. 决策树</h2><hr>\n<p>Decision Tree　树形结构，根据损失函数最小化原则建立决策树模型</p>\n<p><img src=\"/2019/01/31/machine-learning/decision-tree/tree_basic.png\" alt=\"1548939696690\"></p>\n<p>if-then规则集合，每一条路径构建一条规则</p>\n<ul>\n<li><p>内部节点：特征/属性，规则的条件</p>\n</li>\n<li><p>叶节点：　类，规则的结论</p>\n</li>\n</ul>\n<p>条件概率分布</p>\n<p>将特征空间划分为互不相交的单元（cell）或区域（region）</p>\n<p>$P(Y=+1|X=c)&gt;0.5$, 单元cell　属于正类</p>\n<p><img src=\"/2019/01/31/machine-learning/decision-tree/feature-space.png\" alt=\"1548940003249\"></p>\n<p><img src=\"/2019/01/31/machine-learning/decision-tree/1548940132559.png\" alt=\"1548940132559\"></p>\n<p><img src=\"/2019/01/31/machine-learning/decision-tree/1548940093157.png\" alt=\"1548940093157\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Part-4-决策树\"><a href=\"#Part-4-决策树\" class=\"headerlink\" title=\"Part 4. 决策树\"></a>Part 4. 决策树</h2><hr>\n<p>Decision Tree　树形结构，根据损失函数最小化原则建立决策树模型</p>\n<p><img src=\"/2019/01/31/machine-learning/decision-tree/tree_basic.png\" alt=\"1548939696690\"></p>\n<p>if-then规则集合，每一条路径构建一条规则</p>\n<ul>\n<li><p>内部节点：特征/属性，规则的条件</p>\n</li>\n<li><p>叶节点：　类，规则的结论</p>\n</li>\n</ul>\n<p>条件概率分布</p>\n<p>将特征空间划分为互不相交的单元（cell）或区域（region）</p>\n<p>$P(Y=+1|X=c)&gt;0.5$, 单元cell　属于正类</p>\n<p><img src=\"/2019/01/31/machine-learning/decision-tree/feature-space.png\" alt=\"1548940003249\"></p>\n<p><img src=\"/2019/01/31/machine-learning/decision-tree/1548940132559.png\" alt=\"1548940132559\"></p>\n<p><img src=\"/2019/01/31/machine-learning/decision-tree/1548940093157.png\" alt=\"1548940093157\"></p>\n"}],"PostAsset":[{"_id":"source/_posts/machine-learning/decision-tree/1548940093157.png","slug":"1548940093157.png","post":"cjx0hdxnt0001n0fy90pyg85f","modified":0,"renderable":0},{"_id":"source/_posts/machine-learning/decision-tree/1548940125578.png","slug":"1548940125578.png","post":"cjx0hdxnt0001n0fy90pyg85f","modified":0,"renderable":0},{"_id":"source/_posts/machine-learning/decision-tree/1548940132559.png","slug":"1548940132559.png","post":"cjx0hdxnt0001n0fy90pyg85f","modified":0,"renderable":0},{"_id":"source/_posts/machine-learning/decision-tree/feature-space.png","slug":"feature-space.png","post":"cjx0hdxnt0001n0fy90pyg85f","modified":0,"renderable":0},{"_id":"source/_posts/machine-learning/decision-tree/tree_basic.png","slug":"tree_basic.png","post":"cjx0hdxnt0001n0fy90pyg85f","modified":0,"renderable":0},{"_id":"source/_posts/deep-learning/nn_cs231n_note/1560776135809.png","slug":"1560776135809.png","post":"cjx0hdxnl0000n0fyvhy8xs6e","modified":0,"renderable":0},{"_id":"source/_posts/deep-learning/nn_cs231n_note/1560776449074.png","slug":"1560776449074.png","post":"cjx0hdxnl0000n0fyvhy8xs6e","modified":0,"renderable":0},{"_id":"source/_posts/deep-learning/nn_cs231n_note/1560779540116.png","slug":"1560779540116.png","post":"cjx0hdxnl0000n0fyvhy8xs6e","modified":0,"renderable":0},{"_id":"source/_posts/deep-learning/nn_cs231n_note/42.png","slug":"42.png","post":"cjx0hdxnl0000n0fyvhy8xs6e","modified":0,"renderable":0},{"_id":"source/_posts/deep-learning/nn_cs231n_note/aae11de6e6a29f50d46b9ea106fbb02a_hd.png","slug":"aae11de6e6a29f50d46b9ea106fbb02a_hd.png","post":"cjx0hdxnl0000n0fyvhy8xs6e","modified":0,"renderable":0},{"_id":"source/_posts/deep-learning/nn_cs231n_note/d0cbce2f2654b8e70fe201fec2982c7d_hd.png","slug":"d0cbce2f2654b8e70fe201fec2982c7d_hd.png","post":"cjx0hdxnl0000n0fyvhy8xs6e","modified":0,"renderable":0},{"_id":"source/_posts/deep-learning/nn_cs231n_note/e743b6777775b1671c3b5503d7afbbc4_hd.png","slug":"e743b6777775b1671c3b5503d7afbbc4_hd.png","post":"cjx0hdxnl0000n0fyvhy8xs6e","modified":0,"renderable":0},{"_id":"source/_posts/deep-learning/nn_cs231n_note/1560779531173.png","slug":"1560779531173.png","post":"cjx0hdxnl0000n0fyvhy8xs6e","modified":0,"renderable":0},{"_id":"source/_posts/deep-learning/nn_cs231n_note/63fcf4cc655cb04f21a37e86aca333cf_hd.png","slug":"63fcf4cc655cb04f21a37e86aca333cf_hd.png","post":"cjx0hdxnl0000n0fyvhy8xs6e","modified":0,"renderable":0},{"_id":"source/_posts/deep-learning/nn_cs231n_note/ccb56c1fb267bc632d6d88459eb14ace_hd.png","slug":"ccb56c1fb267bc632d6d88459eb14ace_hd.png","post":"cjx0hdxnl0000n0fyvhy8xs6e","modified":0,"renderable":0},{"_id":"source/_posts/deep-learning/nn_cs231n_note/8608c06086fc196228f4dda78499a2d9_hd.png","slug":"8608c06086fc196228f4dda78499a2d9_hd.png","post":"cjx0hdxnl0000n0fyvhy8xs6e","modified":0,"renderable":0},{"_id":"source/_posts/deep-learning/nn_cs231n_note/3.1.1.5.png","slug":"3.1.1.5.png","post":"cjx0hdxnl0000n0fyvhy8xs6e","modified":0,"renderable":0},{"_id":"source/_posts/deep-learning/nn_cs231n_note/677187e96671a4cac9c95352743b3806_hd.png","slug":"677187e96671a4cac9c95352743b3806_hd.png","post":"cjx0hdxnl0000n0fyvhy8xs6e","modified":1,"renderable":0}],"PostCategory":[{"post_id":"cjv2f7asf0009jwfynndvnlbx","category_id":"cjv2f7asi000fjwfyvpn8rsh2","_id":"cjv2f7aso000ljwfyokarjlgy"},{"post_id":"cjx0hdxnl0000n0fyvhy8xs6e","category_id":"cjx0hdxnz0002n0fy4ioqzyrp","_id":"cjx0hdxoq000bn0fyrfkvy3x4"},{"post_id":"cjx0hdxnt0001n0fy90pyg85f","category_id":"cjx0hfedz000088fyec7kilc8","_id":"cjx0hfee1000388fyhkv0dcru"}],"PostTag":[{"post_id":"cjv2f7asa0005jwfyi0bkc62o","tag_id":"cjv2f7ash000cjwfyw5xt2bch","_id":"cjv2f7asi000hjwfy6keqzskd"},{"post_id":"cjv2f7asf0009jwfynndvnlbx","tag_id":"cjv2f7asp000mjwfyc7w6gill","_id":"cjv2f7asq000rjwfyx64sb4wu"},{"post_id":"cjx0hdxnt0001n0fy90pyg85f","tag_id":"cjv2f7asj000kjwfyd5tnlzlw","_id":"cjx0hdxoi0006n0fy588cpqf4"},{"post_id":"cjx0hdxnl0000n0fyvhy8xs6e","tag_id":"cjx0hdxo40003n0fyenebgphc","_id":"cjx0hdxoq000an0fyx7falyqy"},{"post_id":"cjx0hdxnl0000n0fyvhy8xs6e","tag_id":"cjv2f7asj000kjwfyd5tnlzlw","_id":"cjx0hdxor000cn0fyt231tq28"},{"post_id":"cjx0hdxnt0001n0fy90pyg85f","tag_id":"cjx0hfee0000188fykvnx1mlh","_id":"cjx0hfee1000288fydexjtp0d"}],"Tag":[{"name":"git","_id":"cjv2f7ase0007jwfyx2mbx8k7"},{"name":"statistics","_id":"cjv2f7ash000cjwfyw5xt2bch"},{"name":"xiguashu","_id":"cjv2f7asi000gjwfy03z92wvj"},{"name":"hexo-asset-image","_id":"cjv2f7asj000kjwfyd5tnlzlw"},{"name":"math","_id":"cjv2f7asp000mjwfyc7w6gill"},{"name":"dl","_id":"cjx0hdxo40003n0fyenebgphc"},{"name":"ml","_id":"cjx0hfee0000188fykvnx1mlh"}]}}